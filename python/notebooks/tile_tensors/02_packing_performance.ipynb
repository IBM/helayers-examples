{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLcdL7XyUlnO"
   },
   "source": [
    "# How Packing Affects Timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5s5IGHQoUlnQ"
   },
   "source": [
    "## Introduction\n",
    "In this notebook we are going to see how different packing options affect the performance of our code. \n",
    "In this notebook we consider these 3 problems:\n",
    "* Multiply a batch of vectors $x_1, \\ldots, x_n$ by a single vector $w$ (the example from the previous notebook). \n",
    "* Multiply a batch of $n$ vector pairs $\\langle v_1, u_1\\rangle, \\ldots, \\langle v_n, u_n\\rangle$. \n",
    "* Execute a sequence of 4 matrix multiplications: $M_1 \\cdot M_2 \\cdot M_3 \\cdot M_4$\n",
    "</ul>\n",
    "\n",
    "For each of these problems we will measure:\n",
    "<ul>\n",
    "    <li> <i>Latency</i> - The time until receiving a first reply of a batch.</li>\n",
    "    <li> <i>Amortized latency</i> - The amortized time to process a single query in a batch. </li>\n",
    "    <li> <i>Memory</i> - The amount of memory needed to process the batch. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yajTn53oUlnT"
   },
   "source": [
    "We start by importing the required packages for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# For ploting graphs\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For the time/memory measurements\n",
    "from timeit import default_timer\n",
    "\n",
    "from psutil import Process\n",
    "# Utility method for checking memory\n",
    "def get_used_ram(): \n",
    "    mem_MB = Process().memory_info().rss / (1024 * 1024)\n",
    "    return mem_MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpoofRaSUlna"
   },
   "source": [
    "## Step 1. Initializing\n",
    "\n",
    "We first import Helayers and initialize it. This time we use a real CKKS context, implemented over [Microsoft SEAL](https://github.com/microsoft/SEAL)$^1$ to measure real timing and memory usage. We intialize the context to operate over ciphertexts of $2^{14}=16,384$ slots with 128-bits security. Finally, we create an `encoder` that we will later use for encoding and encrypting data.\n",
    "\n",
    "$^1$ HeLayers supports a variety of libraries such as [HELib](https://homenc.github.io/HElib/), [Microsoft SEAL](https://github.com/microsoft/SEAL), [Palisade](https://palisade-crypto.org/), [HEAAN](https://heaan.it/), where we arbitrarily chose SEAL for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhelayers\n",
    "\n",
    "print(\"Imported pyhelayers\", pyhelayers.VERSION)\n",
    "\n",
    "# Initialize a CKKS context with 16,384 slots.\n",
    "requirement = pyhelayers.HeConfigRequirement(\n",
    "    num_slots = 16*1024,            # Number of slots per ciphertext\n",
    "    multiplication_depth = 8,       # Allow 8 levels of multiplications\n",
    "    fractional_part_precision = 40, # Set the precision to 1/2^40.\n",
    "    integer_part_precision = 20,    # Set the largest number to 2^20.\n",
    "    security_level = 128)\n",
    "\n",
    "# Use an HE context that uses Micorsoft SEAL\n",
    "he_context = pyhelayers.SealCkksContext()\n",
    "he_context.init(requirement)\n",
    "print(\"Initialized a context with \", he_context.slot_count(), \" slots\")\n",
    "\n",
    "# Create the Encoder using the context.\n",
    "encoder = pyhelayers.TTEncoder(he_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4U6V5O62Ulnc"
   },
   "source": [
    "<br>\n",
    "\n",
    "## Example 1: Inner product of vectors with a constant vector.\n",
    "\n",
    "In [the CTileTensor_basics tutorial](01_CTileTensor_basics.ipynb), we implemented a linear regression by implementing a dot product operation between two vectors $w, x \\in \\mathbb{R}^d$, where $w$ is the model weights vector and $x$ is the sample on which we performed the inference operation. In many cases, we would like to infer data on a batch of $n$ samples $x_1, \\ldots, x_n \\in \\mathbb{R}^d$, where we compute the $n$ results $y_1, \\ldots, y_n \\in \\mathbb{R}$ ,with $y_i = \\langle x_i, w \\rangle$.\n",
    "\n",
    "We start by defining two utility functions `print_stats` and `genDemoInput`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(s):\n",
    "    \"\"\" Reports measurements parameters\n",
    "        Input: a list of 4 elements s = [latency, amortized latency, total_ram, total_tiles_size], where\n",
    "                 latency          - the latency of some computations,\n",
    "                 amortized latency- the amortized latency of the computations, \n",
    "                 total_ram        - the total used RAM of the current process, \n",
    "                 total_tiles_size - the total tiles tensors memory size\n",
    "    \"\"\"\n",
    "    print(\"Latency = \", s[0], \" seconds.\")\n",
    "    print(\"Amortized latency = \", s[1], \" seconds per input.\")\n",
    "    print(\"RAM of tile tensors = \", (s[3]/1024/1024), \"MB.\")\n",
    "\n",
    "def genDemoInput(m, n, t=None):\n",
    "    \"\"\" Generates an m x n matrix with values [1 ,..., m x n] or an m x n x t tensor with values in [1 ,..., m x n x t] when t is not None.\"\"\"\n",
    "    if t is not None:\n",
    "        return np.arange(1, m*n*t + 1).reshape((m, n, t))\n",
    "    else:\n",
    "        return np.arange(1, m*n + 1).reshape((m, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3XBGcfKUlne"
   },
   "source": [
    "Subsequently, we define the function `genAndEncData`, that gets the dimensions $n$ and $d$, in addition to another parameter $\\alpha$ that controls the packing $0 \\le \\alpha \\le log_2 (slotNum)$. This function, generates $w$ and $X_{n \\times d}=[x_1,\\ldots,x_n]$, pack them by broadcasting $w$ over the 0 axis $n$ times using the function `get_with_duplicated_dim(0)` and returns their encryption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genAndEncData(he_context, encoder, n, d, alpha):\n",
    "    \"\"\" A packing-oblivious function that generates and encrypts the input (X) and the model weights (w).\n",
    "        Input:   n     - number of x input vectors\n",
    "                 d     - dimension of every vector\n",
    "                 alpha - a parameter that controls the packing, 0 <= alpha <= log_2 (slotNum)\n",
    "        \n",
    "        Returns: cW        - an encrypted tile tensor of w\n",
    "                 cX        - a vector of encrypted tile tensor that represent X\n",
    "                 batch_num - number of batches\n",
    "    \"\"\"\n",
    "    assert isinstance(alpha, int),                        \"alpha must be integer\"\n",
    "    assert alpha >= 0,                                    \"alpha must be non-negative\"\n",
    "    assert alpha <= math.log(he_context.slot_count(), 2), \"alpha must be smaller than log_2 n\"\n",
    "\n",
    "    # Generate the input (plaintext) X = [x_1, ... , x_n] and the weight vector w\n",
    "    X = genDemoInput(n, d)\n",
    "    w = genDemoInput(1, d)\n",
    "\n",
    "    # Compute the tiles shape based on alpha \n",
    "    nDim = pow(2, alpha)\n",
    "    dDim = he_context.slot_count() / nDim\n",
    "    batch_num = math.ceil(n/nDim)\n",
    "\n",
    "    # Set the TT dimensions of X and w, where we use the function get_with_duplicated_dim() to broadcast w over the 0 axis n times.\n",
    "    shapeX = pyhelayers.TTShape([nDim, dDim])\n",
    "    shapeW = pyhelayers.TTShape([nDim, dDim]).get_with_duplicated_dim(0)\n",
    "    \n",
    "    # Encode and encrypt w and X = [x1, ..., xn]\n",
    "    cW = encoder.encode_encrypt(shapeW, w)\n",
    "    cX = [encoder.encode_encrypt(shapeX, X[i*nDim : (i+1)*nDim]) for i in range(batch_num)]\n",
    "    \n",
    "    return cW, cX, batch_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dwu2t9vmUlng"
   },
   "source": [
    "Finally, the function `benchmarkLinearRegression` calls `genAndEncData` to generate an encrypted data and benchmark the linear regression computations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmarkLinearRegression(he_context, encoder, n, d, alpha):\n",
    "    \"\"\" A packing-oblivious function that generates the input (X) and the model weights (w) in plaintext,\n",
    "        subsequently, it iteratively encrypts batches of X and infer data from them using a linear regression model.\n",
    "        Input:   n     - number of x inputs\n",
    "                 d     - dimension of every vector\n",
    "                 alpha - a parameter that controls the packing, 0 <= alpha <= log_2 (slotNum)\n",
    "        \n",
    "        Returns: the following measurements data: \n",
    "                 latency          - the latency of the computations,\n",
    "                 Amortized latency- the amortized latency of the computations, \n",
    "                 total_ram        - the total used RAM of the current process, \n",
    "                 total_tiles_size - the total tiles tensors memory size\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate and Encrypt the data\n",
    "    cW, cX, batch_num = genAndEncData(he_context, encoder, n, d, alpha)\n",
    "\n",
    "    # Estimate the memory requirements of the tile tensors. \n",
    "    # Note that we take the maximum over the elements of X because in a real use-case every batch will be encrypted and evaluated separately\n",
    "    # Thus the pick memory equals only the maximal tile tensor size.\n",
    "    estMemW = cW.get_estimated_memory_usage_bytes()\n",
    "    estMemX = max([cX[i].get_estimated_memory_usage_bytes() for i in range(batch_num)])\n",
    "\n",
    "    # Initialize measuring parameters\n",
    "    estMemOutput = 0\n",
    "    startTime = default_timer()\n",
    "\n",
    "    # Compute cOutput = X[i]*w for all batches and measure the latency/throuput\n",
    "    for i in range(batch_num):\n",
    "        cOutput = cX[i]\n",
    "        cOutput.multiply(cW)\n",
    "        \n",
    "        # We measure the output memory usage here because it is reduced after calling get_sum_over_dim.\n",
    "        estMemOutput = max(estMemOutput, cOutput.get_estimated_memory_usage_bytes())\n",
    "\n",
    "        cOutput = cOutput.get_sum_over_dim(1)\n",
    "\n",
    "    # Compute the final results.\n",
    "    raw_time         = default_timer() - startTime\n",
    "    latency          = raw_time / batch_num\n",
    "    amortized_latency= raw_time/n\n",
    "    total_ram        = get_used_ram()\n",
    "    total_tiles_size = estMemW + estMemX + estMemOutput\n",
    "    \n",
    "    return latency, amortized_latency, total_ram, total_tiles_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqc56QYKUlnj"
   },
   "source": [
    "We first set $n=10{,}000$, $d=100$ and $\\alpha=10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(benchmarkLinearRegression(he_context, encoder, 10*1000, 100, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpP4gzorUlnl"
   },
   "source": [
    "Next we note what happens to the runtime and memory when we use $\\alpha=8$ instead of $10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(benchmarkLinearRegression(he_context, encoder, 10*1000, 100, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22HDWxdZUlnp"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "You are required to have latency of at most 0.2 seconds. What is the lowest amortized latency you can achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKikOgWwUlnq"
   },
   "source": [
    "## Example 2: Inner products\n",
    "In this example we get as input 2 lists of $d$-dimensional vectors $u_1, \\ldots, u_n$ and $v_1, \\ldots, v_n$ and compute the dot-product $\\langle u_i, v_i \\rangle$. As before we start by defining the function `genAndEncData2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genAndEncData2(he_context, encoder, n, d, alpha):\n",
    "    \"\"\" A packing-oblivious function that generates and encrypts the input (X) and the model weights (w).\n",
    "        Input:   n     - number of x inputs\n",
    "                 d     - dimension of every vector\n",
    "                 alpha - a parameter that controls the packing, 0 <= alpha <= log_2 (slotNum)\n",
    "        \n",
    "        Returns: cV        - a vector of encrypted tile tensor that represent V\n",
    "                 cU        - a vector of encrypted tile tensor that represent U\n",
    "                 batch_num - number of batches\n",
    "    \"\"\"\n",
    "    assert isinstance(alpha, int),                        \"alpha must be integer\"\n",
    "    assert alpha >= 0,                                    \"alpha must be non-negative\"\n",
    "    assert alpha <= math.log(he_context.slot_count(), 2), \"alpha must be smaller than log_2 n\"\n",
    "\n",
    "    nDim = pow(2, alpha)\n",
    "    dDim = he_context.slot_count() / nDim\n",
    "    batch_num = math.ceil(n/nDim)\n",
    "    \n",
    "    # Note that V and U has the same shape\n",
    "    shapeInput = pyhelayers.TTShape([nDim, dDim])\n",
    "\n",
    "    # Generate the input V = [v_1, ..., v_n] and U = [u_1, ..., u_n] (plaintext)\n",
    "    pV = genDemoInput(n,d)\n",
    "    pU = genDemoInput(n,d)\n",
    "\n",
    "    # Encode and encrypt the data\n",
    "    cV = [encoder.encode_encrypt(shapeInput, pV[i*nDim : (i+1)*nDim]) for i in range(batch_num)]\n",
    "    cU = [encoder.encode_encrypt(shapeInput, pU[i*nDim : (i+1)*nDim]) for i in range(batch_num)]\n",
    "\n",
    "    return cV, cU, batch_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cqFST7EUlnr"
   },
   "source": [
    "Finally, the function `benchmarkInnerProduct` calls `genAndEncData2` to generate an encrypted data and benchmark the linear regression computations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmarkInnerProduct(he_context, encoder, n, d, alpha):\n",
    "    \"\"\" A packing-oblivious function that generates the input (X) and the model weights (w) in plaintext,\n",
    "        subsequently, it iteratively encrypts batches of X and infer data from them using a linear regression model.\n",
    "        Input:   n     - number of x inputs\n",
    "                 d     - dimension of every vector\n",
    "                 alpha - a parameter that controls the packing, 0 <= alpha <= log_2 (slotNum)\n",
    "        \n",
    "        Returns: the following measurements data: \n",
    "                 latency          - the latency of the computations,\n",
    "                 amortized latency- the amortized latency of the computations, \n",
    "                 total_ram        - the total used RAM of the current process, \n",
    "                 total_tiles_size - the total tiles tensors memory size\n",
    "    \"\"\"\n",
    "\n",
    "    cV, cU, batch_num = genAndEncData2(he_context, encoder, n, d, alpha)\n",
    "\n",
    "    # Estimate the memory requirements of the tile tensors\n",
    "    # Note that we take the maximum over the elements of U,V because in a real use-case every batch will be encrypted and evaluated separately\n",
    "    # Thus the pick memory equals only the maximal tile tensor size.\n",
    "    estMemV = max([cV[i].get_estimated_memory_usage_bytes() for i in range(batch_num)])\n",
    "    estMemU = max([cU[i].get_estimated_memory_usage_bytes() for i in range(batch_num)])\n",
    "\n",
    "    estMemOutput = 0\n",
    "    startTime = default_timer()\n",
    "    #print(cU[0].get_shape())\n",
    "    # Compute cOutput = V[i]*U[i] for all batches and measure the latency/throuput\n",
    "    for i in range(batch_num):\n",
    "        cOutput = cU[i]\n",
    "        cOutput.multiply(cV[i])\n",
    "        \n",
    "        # We measure the output memory usage here because it is reduced after calling get_sum_over_dim.\n",
    "        estMemOutput = max(estMemOutput, cOutput.get_estimated_memory_usage_bytes())\n",
    "\n",
    "        cOutput = cOutput.get_sum_over_dim(1)\n",
    "\n",
    "    # Compute the final results.\n",
    "    raw_time         = default_timer() - startTime\n",
    "    latency          = raw_time/batch_num\n",
    "    amortized_latency= raw_time/n\n",
    "    total_ram        = get_used_ram()\n",
    "    total_tiles_size = estMemU + estMemV + estMemOutput\n",
    "\n",
    "    return latency, amortized_latency, total_ram, total_tiles_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(benchmarkInnerProduct(he_context, encoder, 10*1000, 100, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPsgWbCSUlns"
   },
   "source": [
    "Let's plot how the dependency between latency and amortized latency changes for different $\\alpha$ values. For that we first benchmark `benchmarkInnerProduct` on different values of `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphaRange = range(0,15)\n",
    "d = 32\n",
    "\n",
    "counts = np.zeros(shape = [0,4])\n",
    "for alpha in tqdm(alphaRange):\n",
    "    n=pow(2,alpha)\n",
    "    counts = np.append(counts, [benchmarkInnerProduct(he_context, encoder, n,d,alpha)], axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwJw8_54Ulnt"
   },
   "source": [
    "Subsequently we use the function `plotAlphaDependntGraphs` that gets as input the range of $\\alpha$ values and the measurements results and generates 3 plots for latency, amortized latency, and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAlphaDependntGraphs(alphaRange, counts):\n",
    "    latency, amortized_latency, ram, mem = counts.T\n",
    "    \n",
    "    # Plot a graph between alpha and the latency\n",
    "    plt.plot(alphaRange, latency)\n",
    "    plt.xlabel(\"alpha\")\n",
    "    plt.ylabel(\"latency (sec.)\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot a graph between alpha and the latency\n",
    "    plt.plot(alphaRange, amortized_latency)\n",
    "    plt.xlabel(\"alpha\")\n",
    "    plt.ylabel(\"Amortized latency (sec. per input)\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot a graph between alpha and the memory\n",
    "    plt.plot(alphaRange, mem)\n",
    "    plt.xlabel(\"alpha\")\n",
    "    plt.ylabel(\"Ram (bytes)\")\n",
    "    plt.show()\n",
    "\n",
    "plotAlphaDependntGraphs(alphaRange, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8v6CnB6vUlnu"
   },
   "source": [
    "To explain the graphs, note that when $\\alpha=14$ we pack slotCount pairs in our tile tensor, where the $i$-th pair is encoded in the $i$-th slot of the tensors. In this case when we run the inner-product code we do not use rotations. On the other extreme, when $\\alpha=0$, we pack one pair. The tuples of each vector in the pair are mapped to different slots in ciphertexts.\n",
    "For intermediate values $(0 < alpha < 14$) we map several tuples of each pair to different slots of the same ciphertext.\n",
    "\n",
    "#### Explaining the latency graph\n",
    "\n",
    "As expected, at both extremes ($\\alpha=0$ and $\\alpha=14$) the graph has local maxima.\n",
    "When $\\alpha=0$ the execution takes a long time because the inner-product requires rotations to sum over the slots of the product of $v$ and $u$.\n",
    "When $\\alpha=14$, indeed we do not require rotations but we operate over slotCount pairs.\n",
    "\n",
    "#### Explaining the amortized latency graph\n",
    "\n",
    "The amortized time to compute the inner-product reduces as alpha grows. This is expected since as alpha grows we need to perform less rotations, while the amortized number of other gates does not grow.\n",
    "\n",
    "#### Explaining the memory graph\n",
    "\n",
    "The memory needed to compute the inner product grows as alpha grows. This is expected since with higher values of alpha we keep more pairs in memory in each iteration to operate on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-aE7K_hUlnu"
   },
   "source": [
    "## Automatically finding the best packing\n",
    "\n",
    "As you saw in the exercise, finding the best (supported) packing given constrains is a repetitious boring task. The first improvement that comes to mind is writing a script containing a for-loop. When the shape of the TileTensor has more dimensions, naively brute-forcing over the solution space becomes infeasible. To avoid this complexity, HELayers includes a packing-optimizer that:\n",
    "\n",
    "<ul>\n",
    "    <li>supports constrains</li>\n",
    "    <li>supports brute-force and also other heuristics such as gradient descent to find a good solution</li>\n",
    "    <li>supports running in simulation mode where we run the code over plaintext and count gates to estimate the running time for each packing-option</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZB1rGAKUlnv"
   },
   "source": [
    "## Example 3: Matrix-matrix multiplication\n",
    "\n",
    "In this example, we show how to multiply 4 matrices: $M_1 \\cdot M_2 \\cdot M_3 \\cdot M_4$,\n",
    "where the dimensions are as follows:\n",
    "<ul>\n",
    "    <li>$M_1 \\in \\mathbb{R}^{a \\times b}$</li>\n",
    "    <li>$M_2 \\in \\mathbb{R}^{b \\times c}$</li>\n",
    "    <li>$M_3 \\in \\mathbb{R}^{c \\times d}$</li>\n",
    "    <li>$M_4 \\in \\mathbb{R}^{d \\times e}$</li>\n",
    "</ul>\n",
    "\n",
    "To do that efficiently we will introduce a 3rd dimension which is going to be duplicated. The index of the duplicated dimension is different for each matrix. Specifically, we are going to pack the matrices this way:\n",
    "<ul>\n",
    "    <li>$M_1$ is packed as $[b,a,*]$</li>\n",
    "    <li>$M_2$ is packed as $[b,*,c]$</li>\n",
    "    <li>$M_3$ is packed as $[d,*,c]$</li>\n",
    "    <li>$M_4$ is packed as $[d,*,e]$</li>\n",
    "</ul>\n",
    "\n",
    "We pack each matrix as a 3-dimensional tensor. The choice of shapes depend on the matrices we multiply. For example, when multiplying $M_1 \\cdot M_2$:\n",
    "<ul>\n",
    "    <li>We set the mutual matrix dimension ($b$) at the same tensor dimension.</li>\n",
    "    <li>The other matrix dimensions ($a$ and $c$) are placed at different tensor dimensions</li>\n",
    "    <li>Tensor dimension that no matrix dimension was mapped to are made duplicated.</li>\n",
    "    <li>The encoding of $M_1$ seems \"transposed\". This is because placing the mutual matrix dimension ($b$) at the first tensor has advantages.</li>\n",
    "</ul>\n",
    "The shapes of $M_3$ and $M_4$ are similarly chosen to match the shape of the output $(M_1 \\cdot M_2)$ and $((M_1 \\cdot M_2) \\cdot M_3)$.\n",
    "  \n",
    "\n",
    "The result of multiplying $M_1 \\cdot M_2$ is $[b,a,c]$ and after summing over the 1st tensor dimension we get $[*,a,c]$. Multipling it with $M_3$ (whose shape is $[d,*,c]$) we get a tensor whose shape is $[d,a,c]$. After summing over the 3rd tensor dimension we get $[d,a,1?]$ and after broadcasting we get $[d,a,*]$.\n",
    "Multiplying by $M_4$ (whose shape is $[d,*,e]$) we get a tensor whose shape is $[d,a,e]$. After summing over the 1st tensor dimension we get $[*,a,e]$.\n",
    "\n",
    "Note that when summing over the 1st tensor dimension the output is already duplicated, which is why we prefer to encode $M_1$ with a shape that seems transposed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_matrix_matrix_multiplication(a, b, c, d, e, alpha, beta):\n",
    "    assert isinstance(alpha, int) and isinstance(beta, int), \"alpha and beta must be integer\"\n",
    "    assert alpha >= 0 and beta >= 0, \"alpha and beta must be non-negative\"\n",
    "    assert alpha+beta <= math.log(he_context.slot_count(), 2), \"alpha+beta must be smaller than log_2 n\"\n",
    "\n",
    "    t1 = pow(2, alpha)\n",
    "    t2 = pow(2, beta)\n",
    "    t3 = he_context.slot_count() / t1 / t2\n",
    "\n",
    "    shapeFirst = pyhelayers.TTShape([t1, t2, t3]).get_with_duplicated_dim(2)\n",
    "    shapeLater = pyhelayers.TTShape([t1, t2, t3]).get_with_duplicated_dim(1)\n",
    "    \n",
    "    # Generate the input matrix M1 (plaintext)\n",
    "    pM1 = genDemoInput(b, a, 1)\n",
    "    pM2 = genDemoInput(b, 1, c)\n",
    "    pM3 = genDemoInput(d, 1, c)\n",
    "    pM4 = genDemoInput(d, 1, e)\n",
    "\n",
    "    # Encrypt pM1 - pM4\n",
    "    cM1 = encoder.encode_encrypt(shapeFirst, pM1)\n",
    "    cM2 = encoder.encode_encrypt(shapeLater, pM2)\n",
    "    cM3 = encoder.encode_encrypt(shapeLater, pM3)\n",
    "    cM4 = encoder.encode_encrypt(shapeLater, pM4)\n",
    "\n",
    "    estMem = cM1.get_estimated_memory_usage_bytes() + \\\n",
    "             cM2.get_estimated_memory_usage_bytes() + \\\n",
    "             cM3.get_estimated_memory_usage_bytes() + \\\n",
    "             cM4.get_estimated_memory_usage_bytes()\n",
    "\n",
    "    print(f\"M1 shape = \\t\\t\\t\\t\\t{cM1.get_shape()}\")\n",
    "    print(f\"M2 shape = \\t\\t\\t\\t\\t{cM2.get_shape()}\")\n",
    "\n",
    "    startTime = default_timer()\n",
    "    \n",
    "    cOut = cM1\n",
    "    cOut.multiply(cM2)\n",
    "    cOut = cOut.get_sum_over_dim(0)\n",
    "\n",
    "    print(f\"M1 * M2 shape = \\t\\t\\t\\t{cOut.get_shape()}\\n\")\n",
    "    print(f\"M3 shape = \\t\\t\\t\\t\\t{cM3.get_shape()}\")\n",
    "    \n",
    "    cOut.multiply(cM3)\n",
    "    cOut = cOut.get_sum_over_dim(2)\n",
    "    print(f\"M1 * M2 * M3 shape = \\t\\t\\t\\t{cOut.get_shape()}\")\n",
    "    cOut.clear_unknowns()\n",
    "    print(f\"M1 * M2 * M3 shape (After clearing unknowns) = \\t{cOut.get_shape()}\")\n",
    "    cOut.duplicate_over_dim(2)\n",
    "\n",
    "    print(f\"M1 * M2 * M3 shape (After duplicating) = \\t{cOut.get_shape()}\\n\")\n",
    "    print(f\"M4 shape = \\t\\t\\t\\t\\t{cM4.get_shape()}\")\n",
    "        \n",
    "    cOut.multiply(cM4)\n",
    "    cOut = cOut.get_sum_over_dim(0)\n",
    "    \n",
    "    print(f\"M1 * M2 * M3 * M4 shape = \\t\\t\\t{cOut.get_shape()}\\n\")\n",
    "    \n",
    "    # Assert the output contains the expected shape\n",
    "    assert (cOut.get_shape().get_original_sizes()==[1,a,e]).all()\n",
    "\n",
    "    latency = amortized_latency = (default_timer() - startTime)\n",
    "    ram = get_used_ram()    \n",
    "    return latency, amortized_latency, ram, estMem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(benchmark_matrix_matrix_multiplication(4,5,6,7,8,4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhLwHp4CUlnw"
   },
   "source": [
    "\n",
    "## Step 5. Up next\n",
    "\n",
    "In this notebook we explored with various packing options, and saw how they affect performance. HELayers actually has an automatic optimizer, that finds the optimal packing for a given computation. \n",
    "You can see in the parent folder, demo 02, how to read a Keras model with HELayers, find an optimal packing for the input and run it over FHE (CKKS).\n",
    "\n",
    "The next notebook in this tutorial shows how we can use tile tensors to simulate large ciphertexts with smaller ones, simplifying algorithm design and improve performance.\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

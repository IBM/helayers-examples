{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLToolbox Demonstration\n",
    "Expected RAM usage: 40 GB.\n",
    "Expected runtime: 20 minutes. (with debug_mode=True, as will be explained below)\n",
    "\n",
    "NVIDIA A100-SXM4-40GB, 10 CPUs: 80 minutes (with debug_mode=False)\n",
    "\n",
    "Note that it is possible to run the notebook on different hardware platforms. When multiple GPUs are available the notebook will automatically utilize them, to speed up the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This demo notebook explores how to prepare a machine learning model for use with Fully Homomorphic Encryption (FHE) through MLToolbox. MLToolbox provides specialized tools designed to convert models into polynomial representations, all while minimizing performance degradation. In the following sections, we'll delve into the process of adapting models to polynomial form and discuss how MLToolbox simplifies this task.\n",
    "\n",
    "Note: \n",
    " -  Currently supported models: AlexNet, LeNet5, Squeezenet, SqueezenetCHET, ResNet18, ResNet50, ResNet101, ResNet152 and CLIP-RN50. To learn how to add a new model, please refer to: `help(nn_module)`.\n",
    " -  Supported datasets: CIFAR10, CIFAR100, COVID_CT, COVID_XRAY, Places205, Places365 and ImageNet. To add a new dataset, please see `help(DatasetWrapper)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Step 1. Training the base model](#train)            \n",
    "* [Step 2. Transforming the original model into an intermediate, range-aware form](#intermediate)   \n",
    "* [Step 3. Transforming the range-aware form into a polynomial form](#polynomial)   \n",
    "* [Step 4. Encrypting the trained model and predicting over encrypted data](#encrypt)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "## Step 1. Training the base model\n",
    "\n",
    "In this step, we will train the base model using MLToolbox. This base model will serve as the starting point for the gradual conversion process we will undertake later.\n",
    "\n",
    "Note: If you already have a pre-trained model, you can skip this step. Simply ensure that the model is saved as demonstrated in [the steps below](#save)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Start with some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#Printing only error debug printouts\n",
    "os.environ[\"LOG_LEVEL\"]=\"ERROR\"\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import utils # used for benchmarking\n",
    "#trange used to show progress of training loops\n",
    "from tqdm.notebook import trange\n",
    "#magic function that renders the figure in a notebook\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "#FHE related import\n",
    "import pyhelayers\n",
    "#mltoolbox imports\n",
    "from pyhelayers.mltoolbox.arguments import Arguments\n",
    "import pyhelayers.mltoolbox.utils.util as util\n",
    "from pyhelayers.mltoolbox.poly_activation_converter import starting_point\n",
    "from pyhelayers.mltoolbox.data_loader.ds_factory import DSFactory\n",
    "from pyhelayers.mltoolbox.model.DNN_factory import DNNFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Convert dataset into ffcv format\n",
    "\n",
    "We use the ffcv library to speed up training (https://docs.ffcv.io/index.html). Beyond just acceleration, the ffcv library introduces unique data transformations not found in Pytorch. One notable transformation is the cutout transformation (https://arxiv.org/abs/1708.04552), which has been shown to enhance performance.\n",
    "It is possible to not use the ffcv library with mltoolbox, in such a case the `args.ffcv argument`, that will be presented below, should be set to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ffcv.writer import DatasetWriter\n",
    "from ffcv.fields import RGBImageField, IntField\n",
    "from torchvision import transforms,datasets\n",
    "\n",
    "path = 'outputs/mltoolbox/train.beton'\n",
    "writer = DatasetWriter(path, {'image': RGBImageField(),'label': IntField()})\n",
    "ds = datasets.CIFAR10('outputs/mltoolbox/cifar_data', train=True, download=True)\n",
    "writer.from_indexed_dataset(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Initialize the trainer object.\n",
    "In the following cell, we initialize the `Arguments` class. This class defines the default values for various parameters related to the training process, allowing for their customization. \n",
    "`model`, `dataset_name`, `num_epochs`, `classes` and `data_dir` are required arguments and do not have default values.\n",
    "The `data_dir` argument specifies the dataset location.\n",
    "\n",
    "If you are using a CPU for training, debug_mode is automatically set to True. This enables the model to be trained on a small subset of the dataset and for only one epoch, allowing you to test your setup quickly.\n",
    "When CUDA is available, debug_mode is automatically set to False to perform the actual, full-scale training process.\n",
    "\n",
    "MLToolbox uses a fixed seed by default, to ensure reproducibility. Randomicity is achieved by setting `args.seed` to different values. Still fluctuations in the results are possible, when running the code on a different architecture.\n",
    "\n",
    "A pre-trained model checkpoint can be used, by setting args.from_checkpoint to the checkpoint location. It is also possible to run the training from scratch by setting: args.from_checkpoint = ''\n",
    "\n",
    "The `starting_point` method receives the user arguments and returns two objects: `trainer` and `poly_activation_converter`, which will assist in converting the original model into a polynomial form using the following steps.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_mode = not torch.cuda.is_available()\n",
    "if debug_mode:\n",
    "    num_epochs = 1\n",
    "    batch_size = 10\n",
    "else:\n",
    "    num_epochs = 30\n",
    "    batch_size = 500\n",
    "    \n",
    "args = Arguments(model=\"resnet18\", dataset_name=\"CIFAR10_224\", num_epochs=num_epochs, classes=10, data_dir = 'cifar_data')\n",
    "\n",
    "#After initializing an `Argument` object it is possible to customize its settings\n",
    "args.lr=0.05\n",
    "args.opt = \"sgd\"\n",
    "args.batch_size = batch_size\n",
    "args.ffcv = True\n",
    "args.ffcv_train_data_path = path\n",
    "args.pooling_type = 'max'\n",
    "\n",
    "baseline_chp_location = os.path.join(utils.get_data_sets_dir(), 'mltoolbox', 'resnet18_baseline_checkpoint.pth.tar')\n",
    "args.from_checkpoint = baseline_chp_location\n",
    "\n",
    "#Used to do a quick test run. When performing a real run, either remove this argument, or set it to `False` \n",
    "args.debug_mode = debug_mode\n",
    "\n",
    "#Use the following line to utilize the arguments:\n",
    "trainer, poly_activation_converter, _ = starting_point(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.get_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The supported datasets can be observed using the following call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DSFactory.print_supported_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The supported models can be observed using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNNFactory.print_supported_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Training the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training loop using the trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This list will accumulate the validation accuracy for each epoch, so we can later plot it\n",
    "val_acc = []\n",
    "\n",
    "# Run the training loop if the model was not loaded from checkpoint\n",
    "if not args.from_checkpoint:\n",
    "        epochs_range = trange(1,args.num_epochs + 1)\n",
    "        scheduler = ReduceLROnPlateau(trainer.get_optimizer(), factor=0.5, patience=3, min_lr=0.000001, verbose=True)\n",
    "        for epoch in epochs_range:\n",
    "                trainer.train_step(args, epoch, epochs_range)\n",
    "                val_metrics, val_cf = trainer.validation(args, epoch) # perform validation. Returns metrics (val_metrics) and confusion matrix (val_cf)\n",
    "                val_acc.append(val_metrics.get_avg('accuracy'))\n",
    "                \n",
    "                scheduler.step(val_metrics.get_avg('loss'))\n",
    "                \n",
    "        # Saving the model\n",
    "        # The save location is defined by the args.save_dir argument. The default value is set to outputs/mltoolbox.\n",
    "        util.save_model(trainer, poly_activation_converter, args, val_metrics, epoch, val_cf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the test metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics, test_cf = trainer.test(args, 1)\n",
    "print({'loss': test_metrics.get_avg('loss'), 'accuracy': test_metrics.get_avg('accuracy')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a trained model that can be converted to a polynomial form later on. The accuracy of this model is 0.962. Once the model is ready, we save it to a file for future use.\n",
    "\n",
    "Note: as mentioned earlier, you don't have to use a model trained with MLToolbox. You can use a pre-trained model and skip Step 1 of this notebook – just make sure your model is saved in the supported form `{'model': model}` as shown below, and that it is supported by mltoolbox, or extended:\n",
    "\n",
    "```\n",
    "state = {'model': model} \n",
    "torch.save(state, file_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(val_acc):\n",
    "    plt.plot([*range(1, args.num_epochs + 1)], val_acc)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xticks(np.arange(1,args.num_epochs + 1,step=2)) \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "#if we run the training, the validation accuracy graph can be plotted\n",
    "if val_acc:\n",
    "    plot(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intermediate\"></a>\n",
    "## Step 2. Transforming the original model into an intermediate, range-aware form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we aim to minimize the input range to the non-polynomial activation layers. Particularly, this process involves the ReLU activation layers keeping track of the input range. Additionally, a regularization term is added to the network, penalizing values that fall outside of this range.\n",
    "\n",
    "There are two primary reasons for performing this step:\n",
    "\n",
    "- To enable the approximation of activations by polynomials, it is essential to provide the range of inputs to the activation function beforehand.\n",
    "- Our goal is to optimize the input range for more accurate activation approximation using low-degree polynomials. While we use the interval [-10, 10] as an example, the actual range depends on the specific model and data. It is preferable to have a smaller range to achieve a more precise approximation. However, a significantly larger range could negatively impact the accuracy of the polynomial model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Define the arguments to represent what we want to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug_mode:\n",
    "    num_epochs = 1\n",
    "    batch_size = 10\n",
    "else:\n",
    "    num_epochs = 13\n",
    "    batch_size = 200\n",
    "    \n",
    "args = Arguments(model=\"resnet18\", dataset_name=\"CIFAR10_224\", num_epochs=num_epochs, classes=10, data_dir = 'cifar_data')\n",
    "\n",
    "args.opt = \"sgd\"\n",
    "args.pooling_type = \"avg\"\n",
    "args.activation_type= \"relu_range_aware\"\n",
    "args.lr=0.005\n",
    "args.batch_size = batch_size\n",
    "args.range_awareness_loss_weight=0.002\n",
    "args.range_aware_train = True\n",
    "args.save_dir = \"outputs/mltoolbox/range_aware\"\n",
    "args.ffcv = True\n",
    "args.ffcv_train_data_path = path\n",
    "args.debug_mode = debug_mode\n",
    "\n",
    "baseline_chp_location = os.path.join(utils.get_data_sets_dir(), 'mltoolbox', 'resnet18_baseline_checkpoint.pth.tar')\n",
    "args.from_checkpoint = baseline_chp_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `args.opt=\"sgd\"`: The optimizer to be used (the other option is `'adam'`, which is the default)\n",
    "\n",
    "* `args.pooling_type = \"avg\"`: All the pooling operations will be replaced by average-pooling (the default is `\"avg\"`).\n",
    "\n",
    "* `args.activation_type=\"relu_range_aware\"`: The Relu activations will be account for it's input range (other options are `'trainable_poly'`, `'non_trainable_poly'`, `'approx_relu'`, `'relu'`, `'weighted_relu'`, `'relu_range_aware'`,`'square'`).\n",
    "\n",
    "* `args.lr=0.005`: Learning rate\n",
    "\n",
    "* `args.batch_size=batch_size`: Batch size\n",
    "\n",
    "* `args.range_awareness_loss_weight=0.002`: This is the weight that defines how much attention is given to diminishing the ranges during training, relatively to the CrossEntropyLoss. This value needs to be tuned for the used model and data such that the training does not suffer from too hursh accuracy degradation.\n",
    "\n",
    "* `args.range_aware_train=True`: A flag that makes the training be range aware. Can be turned off (The default value is False).\n",
    "\n",
    "* `args.save_dir = \"outputs/mltoolbox/range_aware\"`: The directory where the outputs of this step will be saved.\n",
    "\n",
    "* `args.ffcv = True`: Use ffcv library during training, to optimize speed. In addition the ffcv adds a cutout transformation, which improves the accuracy (The default value is False).\n",
    "\n",
    "* `args.ffcv_train_data_path = path`: The name and location of the ffcv converted data.\n",
    "\n",
    "* `args.debug_mode = debug_mode`: When the debug mode is set to True, the training uses a small subset of the data. The default value is False.\n",
    "\n",
    "* `args.from_checkpoint = baseline_chp_location`: The checkpoint to load the model from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Run starting_point again, with the new arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer, poly_activation_converter, epoch = starting_point(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Replacing max-pooling\n",
    "\n",
    "FHE does not natively support max pooling operations. To address this limitation, we replace max pooling with average pooling. There are two approaches to accomplish this: 1) by training the base model from scratch with average pooling; 2) by converting the pooling operation to average pooling after the model has been trained, and then continuing training for a few additional epochs. To configure MLToolbox to option #2, set the `pooling_type` argument to `'avg'` and run the `make_fhe_friendly` method, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.get_model()\n",
    "model.module.make_fhe_friendly(add_bn=False, pooling_type=args.pooling_type) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Transform and train the range-aware model\n",
    "\n",
    "We train the model for several more epochs. At the beginning of the training loop, the `replace_activations` function handles anything that needs to be replaced in the current epoch. Then, the train step is called. This is the same train step we ran before; the only difference is that the loss function will now have an extra term that will regulize the input, striving to bring them towards the required range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = []\n",
    "ranges_train = []\n",
    "ranges_val = []\n",
    "scheduler = ReduceLROnPlateau(trainer.get_optimizer(), factor=0.5, patience=2, min_lr=args.min_lr, verbose=True)\n",
    "epochs_range = trange(1,args.num_epochs + 1)\n",
    "\n",
    "for epoch in epochs_range:\n",
    "    poly_activation_converter.replace_activations(trainer, epoch, scheduler)\n",
    "    trainer.train_step(args, epoch, epochs_range)\n",
    "    ranges_train.append(trainer.get_all_ranges(args))\n",
    "    \n",
    "    val_metrics, val_cf = trainer.validation(args, epoch) # perform validation. Returns metrics (val_metrics) and confusion matrix (val_cf)\n",
    "    val_acc.append(val_metrics.get_avg('accuracy'))\n",
    "    ranges_val.append(trainer.get_all_ranges(args))\n",
    "    \n",
    "util.save_model(trainer, poly_activation_converter, args, val_metrics, epoch, val_cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting range-aware model that we trained exhibits an overall accuracy of 0.96. We set the `range_aware_weight` to 0.001. When running your model, the `range_aware_weight` should be carefully tuned. While some degradation in this step is expected, it's important to ensure the accuracy doesn't decrease too significantly when working with the ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics, test_cf = trainer.test(args, epoch)\n",
    "print({'loss': test_metrics.get_avg('loss'), 'accuracy': test_metrics.get_avg('accuracy')})\n",
    "\n",
    "plot(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.get_model())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's observe the ranges of the model's activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ranges(ranges):\n",
    "    num_epochs = len(ranges)\n",
    "    num_items = len(ranges[num_epochs-1])\n",
    "\n",
    "    # Select a specific color map\n",
    "    color_map = cm.get_cmap('tab10')\n",
    "    start_epoch = next((i for i, epoch in enumerate(ranges) if epoch), None)\n",
    "    \n",
    "    if start_epoch is None:\n",
    "        print(\"No ranges data found.\")\n",
    "        return\n",
    "    \n",
    "    num_epochs_to_plot = num_epochs - start_epoch\n",
    "    for item_index in range(num_items):\n",
    "        min_values = [epoch[item_index][0] for epoch in ranges[start_epoch:]]\n",
    "        max_values = [epoch[item_index][1] for epoch in ranges[start_epoch:]]\n",
    "        # Generate color index based on item index\n",
    "        color_index = item_index % color_map.N\n",
    "        \n",
    "        plt.plot(range(num_epochs_to_plot), min_values, label=f'Min- {item_index+1}', color=color_map(color_index))\n",
    "        plt.plot(range(num_epochs_to_plot), max_values, label=f'Max- {item_index+1}', color=color_map(color_index))\n",
    "\n",
    "    #plt.legend(loc='upper right')  # Display legend\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Epoch')  # Set x-axis label\n",
    "    plt.ylabel('Value')  # Set y-axis label\n",
    "    plt.title('Minimum and Maximum Values over Epochs')  # Set plot title\n",
    "    \n",
    "    # Set x-axis limits to start from start_epoch\n",
    "    plt.xlim(start_epoch, num_epochs_to_plot)\n",
    "    \n",
    "    plt.show()  # Display the plot\n",
    "    \n",
    "plot_ranges(ranges_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_ranges(ranges_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the ranges are below [-10,10], while one activation still has larger ranges, around [-10,10]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"polynomial\"></a>\n",
    "## Step 3. Transforming the range-aware form into a polynomial form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Relu activations are replaced according to the arguments we've defined.\n",
    "We start with a partially transformed model; the pooling type is already average, and some batch normalization may have been added.\n",
    "Relu is replaced by a non-trainable polynomial, that approximate the RELU in the range that it holds. The remaining epochs are used to improve the model with no additional changes.\n",
    "\n",
    "At the beginning of the training loop, the replace_activations function handles anything that needs to be replaced in the current epoch. Then, the train step is called. This is the same train step we ran before; the only difference is that the loss function has an extra term that regulizes the inputs in the required range.\n",
    "\n",
    "After the training loop has completed, we save the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.pooling_type = \"avg\"\n",
    "args.activation_type= \"non_trainable_poly\"\n",
    "args.batch_size = 200\n",
    "args.num_epochs = 25\n",
    "args.lr=0.005\n",
    "args.from_checkpoint = \"outputs/mltoolbox/range_aware/resnet18_last_checkpoint.pth.tar\"\n",
    "args.range_awareness_loss_weight=0.1\n",
    "args.range_aware_train = True\n",
    "args.save_dir = \"outputs/mltoolbox/polynomial\"\n",
    "\n",
    "if debug_mode:\n",
    "    args.num_epochs = 1\n",
    "    args.batch_size = 10\n",
    "    \n",
    "trainer, poly_activation_converter, epoch = starting_point(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = []\n",
    "ranges_train = []\n",
    "ranges_val = []\n",
    "scheduler = ReduceLROnPlateau(trainer.get_optimizer(), factor=0.5, patience=2, min_lr=args.min_lr, verbose=True)\n",
    "epochs_range = trange(1,args.num_epochs + 1)\n",
    "\n",
    "for epoch in epochs_range:\n",
    "    poly_activation_converter.replace_activations(trainer, epoch, scheduler)\n",
    "    trainer.train_step(args, epoch, epochs_range)\n",
    "    ranges_train.append(trainer.get_all_ranges(args))\n",
    "    \n",
    "    val_metrics, val_cf = trainer.validation(args, epoch) # perform validation. Returns metrics (val_metrics) and confusion matrix (val_cf)\n",
    "    val_acc.append(val_metrics.get_avg('accuracy'))\n",
    "    ranges_val.append(trainer.get_all_ranges(args))\n",
    "    \n",
    "util.save_model(trainer, poly_activation_converter, args, val_metrics, epoch, val_cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting accuracy we got is 0.962"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ranges(ranges_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ranges(ranges_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Convert the model into onnx format.\n",
    "The fhe_best checkpoint, that was saved by util.save_model is read and the model is converted into onnx (some format changes are applyed to the model during the convertion.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path, model = util.save_onnx(args, poly_activation_converter, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below table summarizes the results achieved in this notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Technique    | Accuracy   | \n",
    "|:-------------|:-----------|\n",
    "| **ReLU**     | **0.962**       |\n",
    "| range-aware polynomial  | 0.962     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"encrypt\"></a>\n",
    "## Step 4. Encrypting the trained model and predicting over encrypted data\n",
    "We now encrypt the polynomial model trained above and use the encrypted model to make predictions on encrypted data. We'll also make predictions using the plain (unencrypted) model, and compare the two sets of results. This is demonstrated in a different notebook. See `misc/19_MLToolbox_FHE.ipynb` for more details.  \n",
    "We save the input samples and the expected output labels for the prediction process of notebook `misc/19_MLToolbox_FHE.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 10\n",
    "trainer, poly_activation_converter, epoch = starting_point(args)\n",
    "\n",
    "plain_samples, labels = next(iter(trainer.val_generator))\n",
    "torch.save(plain_samples, 'outputs/mltoolbox/plain_samples.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moran Baruch, Nir Drucker, Gilad Ezov, Eyal Kushnir, Jenny Lerner, Omri Soceanu, Itamar Zimerman. \"Sensitive Tuning of Large Scale CNNs for E2E Secure Prediction using Homomorphic Encryption\" (2023)\n",
    "https://arxiv.org/abs/2304.14836"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e4a86c6de552a75de3f76546b88fdc07403a59a0ff9bc5c3fe3ef6120ab2c02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

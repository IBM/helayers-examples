{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d443513",
   "metadata": {},
   "source": [
    "# Text classification Using FHE for the *20 Newsgroups* dataset\n",
    "\n",
    "## Introduction\n",
    "This tutorial is based on the 20-newsgroups dataset and classifies a text snippet to its relevant category (snippet is represented as a bag of words vector with tf/idf scores). The tutorial is composed of two parts: the first part shows how to train the unencrypted model and the second part shows how to transform the plaintext model into an encrypted one and how to perform an FHE inference with it. The classifier model is a neural network (NN) with a single hidden layer and polynomial activation. In this tutorial, we used only 4 out of 20 available categories.  \n",
    "\n",
    "#### This demo uses the 20 Newsgroups dataset, originally taken from: http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html\n",
    "\n",
    "This data set consists of 20000 messages taken from 20 newsgroups.\n",
    "\n",
    "## Use case\n",
    "A potential use case for text classification is sentiment analysis. For example, take a scenario where a call center has the texts from customer calls and they would like to categorize it (e.g. loans, general complaints, investments, etc.) but the calls are considered to be sensitive data. With FHE, you can encrypt the text and categorize the calls in order to analyze customer sentiment - all while preserving customer privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b4fab",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Step 1. Load and train the unencrypted model\n",
    "In this step, you load and train the model without using FHE - this is just data science. You take the dataset and separate it into a training set and a test set. We will use FHE in Step 2 to perform the inference.\n",
    "\n",
    "\n",
    "#### 1a. Load, train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16008875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import utils \n",
    "\n",
    "utils.verify_memory()\n",
    "\n",
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'soc.religion.christian', \n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "remove = ('headers','footers','signatures')\n",
    "print(f\"Loading 20 newsgroups dataset for categories: {categories}\")\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "print('Data loaded')\n",
    "\n",
    "# order of labels in `target_names` can be different from `categories`\n",
    "target_names = data_train.target_names\n",
    "\n",
    "# split a training set and a test set\n",
    "y_train, y_test = data_train.target, data_test.target\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d9bd78",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 1b. Preprocess\n",
    "Here you try to select the best features for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea8ee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "\n",
    "X_train = vectorizer.fit_transform(data_train.data)\n",
    "\n",
    "print(\"Extracting features from the test data using the same vectorizer\")\n",
    "X_test = vectorizer.transform(data_test.data)\n",
    "print()\n",
    "\n",
    "# mapping from integer feature name to original token string\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "if len(feature_names) > 0:\n",
    "    feature_names = np.asarray(feature_names)\n",
    "\n",
    "n_features = 3000\n",
    "print(f\"Extracting {n_features} best features by a chi-squared test\")\n",
    "ch2 = SelectKBest(chi2, k=n_features)\n",
    "X_train = ch2.fit_transform(X_train, y_train)\n",
    "X_test = ch2.transform(X_test)\n",
    "\n",
    "# Convert class vector to binary class matrices\n",
    "num_classes = len(categories)\n",
    "y_train_cat = utils.to_categorical(y_train, num_classes)\n",
    "y_test_cat = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c46889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize test data that will be used for evaluation during FHE inference \n",
    "from utils import save_data_set, serialize_model\n",
    "import os \n",
    "\n",
    "PATH = os.path.join('.', 'data', 'text_classification')\n",
    "save_data_set(X_test.A, y_test_cat, data_type='test', path=PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98cef46",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 1c. Define the model architecture\n",
    "Here we define the model architecture for the training and the activation function. To make the model HE-friendly, we used a scaled polynomial of a second degree: \\$activation(x)= 0.01\\cdot x^2\\ + x$. This activation funtion, unlike previous examples, isn't just square activation. This is different to show that you can use different activiation functions with FHE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dd0700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(300, input_shape=(X_train.shape[1],))) \n",
    "    model.add(PolyActivation([0.01,1.,0.]))\n",
    "    model.add(Dense(4))  \n",
    "    sgd = SGD(learning_rate=0.1)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), optimizer=sgd, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941cf371",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 1d. Train the model and determine accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fc99aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_val, y_val, num_classes, mode='val'):\n",
    "    y_train_cat = utils.to_categorical(y_train, num_classes)\n",
    "    y_val_cat = utils.to_categorical(y_val, num_classes)\n",
    "    \n",
    "    model.fit(X_train, y_train_cat,\n",
    "              batch_size=200,\n",
    "              epochs=75,\n",
    "              verbose=0,\n",
    "              validation_data=(X_val, y_val_cat),\n",
    "              shuffle=True,\n",
    "              )\n",
    "    score = model.evaluate(X_val, y_val_cat, verbose=0)\n",
    "    print(f'{mode} loss: { score[0]:.3f}')\n",
    "    print(f'{mode} accuracy: {score[1] * 100:.3f}%')\n",
    "    \n",
    "    return score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac3f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from data_gen.activations import PolyActivation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "res = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(skf.split(X_train.A, y_train)):\n",
    "    print (f\"Running Fold {i+1}/{n_folds}\")\n",
    "    \n",
    "    model = None # Clearing the NN.\n",
    "    model = create_model()\n",
    "    res.append(train_and_evaluate_model(model, X_train.A[train_index], y_train[train_index], X_train.A[val_index], y_train[val_index],num_classes))\n",
    "\n",
    "print(f\"Validation results: {res}\")\n",
    "print(f\"Mean validation accuracy:{np.mean(res):.3f}, standard deviation: {np.std(res):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42d41d0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 1e. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d5ef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "model = create_model()\n",
    "acc = train_and_evaluate_model(model, X_train.A, y_train, X_test.A, y_test,num_classes, mode='test')\n",
    "y_pred = np.argmax(model.predict(X_test.todense()), axis=-1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a07e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model serialization\n",
    "from utils import save_data_set, serialize_model\n",
    "import os \n",
    "\n",
    "\n",
    "PATH = os.path.join('.', 'data', 'text_classification')\n",
    "serialize_model(model, PATH, s='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edf817e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "## Step 2. FHE inference\n",
    "\n",
    "\n",
    "#### 2a. Load the plain model and the model weights that we just trained and encrypt them in a trusted environment.\n",
    " We now load the model files in HElayers. This runs internally an optimization process that finds the best parameters for this model. We can provide various additional parameters as input. The only thing we tune here is the batch size (how many samples would you provide each time for the inference model to do the classification). Here we define it as 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import pyhelayers\n",
    "import utils\n",
    "\n",
    "he_run_req = pyhelayers.HeRunRequirements()\n",
    "he_run_req.set_he_context_options([pyhelayers.DefaultContext()])\n",
    "he_run_req.optimize_for_batch_size(8)\n",
    "\n",
    "nn = pyhelayers.NeuralNet()\n",
    "nn.encode_encrypt([PATH + \"/model.json\", PATH + \"/model.h5\"], he_run_req)\n",
    "context = nn.get_created_he_context()\n",
    "batch_size = nn.get_profile().get_optimal_batch_size()\n",
    "print(\"model loaded and encrypted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf4cf77",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 2b. Load and encrypt the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d5ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(PATH + \"/x_test.h5\") as f:\n",
    "    x_test = np.array(f[\"x_test\"])\n",
    "with h5py.File(PATH + \"/y_test.h5\") as f:\n",
    "    y_test = np.array(f[\"y_test\"])\n",
    "    \n",
    "plain_samples, labels = utils.extract_batch(x_test, y_test, batch_size, 0)\n",
    "print('Batch of size',batch_size,'loaded')\n",
    "\n",
    "model_io_encoder = pyhelayers.ModelIoEncoder(nn)\n",
    "samples = pyhelayers.EncryptedData(context)\n",
    "model_io_encoder.encode_encrypt(samples, [plain_samples])\n",
    "print('Test data encrypted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bd14de",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 2c. Perform the FHE inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffd30c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pyhelayers.EncryptedData(context)\n",
    "utils.start_timer()\n",
    "nn.predict(predictions, samples)\n",
    "duration=utils.end_timer('predict')\n",
    "utils.report_duration('predict per sample',duration/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8394ac23",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 2d. Decrypt and print the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c2dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_predictions = model_io_encoder.decrypt_decode_output(predictions)\n",
    "print('predictions',plain_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f6eb9b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 2e. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e33a50d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.assess_results(labels, plain_predictions)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df1dc8a95496e2ea0668c040193b5c5e80cabbb7"
   },
   "source": [
    "### A fully connected network for fraud detection. \n",
    "#### This demo uses the Credit Card Fraud Detection dataset, originally taken from: https://www.kaggle.com/mlg-ulb/creditcardfraud. The notebook is based on a notebook implemented by the Kaggle community (https://www.kaggle.com/omkarsabnis/credit-card-fraud-detection-using-neural-networks), while some changes was applied to make the network's architecture FHE friendly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a391e57ea916bce5bb32ccb58455f379a925a170"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "##### For reproducibility\n",
    "seed_value= 1\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "from tensorflow.keras import backend as K\n",
    "#####\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "import h5py\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "#####\n",
    "# import utils\n",
    "import sys\n",
    "path_to_utils='..'\n",
    "sys.path.append(path_to_utils)\n",
    "import utils\n",
    "# import activations\n",
    "from activations import SquareActivation\n",
    "\n",
    "\n",
    "PATH = os.path.join('..', 'data', 'net_fraud')\n",
    "if not os.path.exists(PATH):\n",
    "    os.makedirs(PATH)\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 32 \n",
    "optimizer = Adam\n",
    "lr = 0.01\n",
    "print(\"misc. init complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(utils.get_data_sets_dir(path_to_utils), 'net_fraud', 'creditcard.csv'))\n",
    "\n",
    "print(f'Reading {df.shape[0]} samples')\n",
    "\n",
    "X = df.loc[:, df.columns.tolist()[1:30]].values\n",
    "Y = df.loc[:, 'Class'].values\n",
    "print(f'number of features: {X.shape[1]}')\n",
    "\n",
    "X = preprocessing.normalize(X)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, stratify=Y, random_state=0)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.33, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicate the smallest class for balancing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replicate_smallest_class(x, y, class_id):\n",
    "        y_fraud_list = y[y == class_id]\n",
    "        x_fraud_list = x[y == class_id]\n",
    "\n",
    "        for _ in range(5):\n",
    "            copy_fraudlist = np.copy(x_fraud_list)\n",
    "            y_fraud_copy = np.copy(y_fraud_list)\n",
    "            x = np.concatenate((x, copy_fraudlist))\n",
    "            y = np.concatenate((y, y_fraud_copy))\n",
    "\n",
    "        permut = np.random.permutation(x.shape[0])\n",
    "        x = x[permut]\n",
    "        y = y[permut]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "x_train, y_train = replicate_smallest_class(x_train, y_train, class_id=1)\n",
    "\n",
    "nb_train_samples = (x_train.shape[0] // batch_size) * batch_size\n",
    "x_train = x_train[:nb_train_samples]\n",
    "y_train = y_train[:nb_train_samples]\n",
    "\n",
    "\n",
    "print(\"After replicating items from the smaller class:\")\n",
    "print(f'x_train: {x_train.shape}')\n",
    "print(f'x_val: {x_val.shape}')\n",
    "print(f'x_test: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(y_train.shape[0], -1)\n",
    "y_val = y_val.reshape(y_val.shape[0], -1)\n",
    "y_test = y_test.reshape(y_test.shape[0], -1)\n",
    "\n",
    "print(\"Training data ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_set(x, y, data_type, s=''):\n",
    "    print(\"Saving x_{} of shape {}\".format(data_type, x.shape))\n",
    "    xf = h5py.File(os.path.join(PATH, f'x_{data_type}{s}.h5'), 'w')\n",
    "    xf.create_dataset('x_{}'.format(data_type), data=x)\n",
    "    xf.close()\n",
    "\n",
    "    yf = h5py.File(os.path.join(PATH, f'y_{data_type}{s}.h5'), 'w')\n",
    "    yf.create_dataset(f'y_{data_type}', data=y)\n",
    "    yf.close()\n",
    "    \n",
    "save_data_set(x_test, y_test, data_type='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fraud Detection Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(20, input_shape=(x_train.shape[1],)))\n",
    "model.add(SquareActivation())\n",
    "model.add(Dense(5))\n",
    "model.add(SquareActivation())\n",
    "model.add(Dense(1))\n",
    "model.add(SquareActivation())\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer(lr=lr),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=2,\n",
    "              validation_data=(x_val, y_val),\n",
    "              shuffle=True,\n",
    "              )\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Test loss: {score[0]:.3f}')\n",
    "print(f'Test accuracy: {score[1] * 100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix - TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "x_test = x_test[0:batch_size,:]\n",
    "y_test = y_test[0:batch_size,:]\n",
    "\n",
    "y_pred_vals = model.predict(x_test)\n",
    "y_pred = (y_pred_vals > 0.5).astype(\"int32\")\n",
    "f,t,thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(f\"AUC Score: {metrics.auc(f,t):.3f}\")\n",
    "print(\"Classification report:\")\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialize model and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(os.path.join(PATH, 'model.json'), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(os.path.join(PATH, 'model.h5'))\n",
    "print(\"Saved model to \",PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

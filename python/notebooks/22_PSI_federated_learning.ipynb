{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ae97fe9",
   "metadata": {},
   "source": [
    "# Private Set Intersection for Vertical Federated Learning using FHE\n",
    "Expected RAM usage: 38 GB  \n",
    "Expected runtime: 10-20 minutes  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4351ebe6",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This example demonstrates a Private Set Intersection (PSI) protocol between three parties using a fourth party, called the Aggregator, to be used for Vertical Federated Learning (VFL). The three parties have their database with private samples. Each sample contains a unique identifier (UID) and a number of features, each is a real number in the range [0,1].\n",
    " \n",
    "The objective is that each party will get by the end of the protocol a CTileTensor encrypted under the aggregator's key that contains only the samples that are in the intersection (i.e., samples that appear at the dataset of each of the parties), without disclosing any information about the other parties' samples or about the size of the intersection. Specifically, each party should not learn whether a specific sample is in the intersection or not, or even the size of the intersection. The CTileTensor then can be further used by each of the parties in the federated learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758da198",
   "metadata": {},
   "source": [
    "## Step 1. Framework Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64efef6b",
   "metadata": {},
   "source": [
    "### 1.1. Start with some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924a4556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhelayers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import utils "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832b7b14",
   "metadata": {},
   "source": [
    "### 1.2. Parties setup\n",
    "In our framework, the parties need to have a 128-bit shared secret that is hidden from the aggregator. The method by which this secret is generated and distributed between the parties is out of the scope of this demo. This secret will be used to mask the intermediate calculations from the aggregator.\n",
    "\n",
    "Furthermore, each party should have a public unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36648c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 128-bit secret that is shared between all the parties and is hidden from the aggregator.\n",
    "parties_shared_secret = [12345,67890]\n",
    "\n",
    "num_parties = 3\n",
    "list_parties = ['party-{}'.format(i+1) for i in range(num_parties)]\n",
    "\n",
    "# Each party should have a public unique ID\n",
    "dict_party_rts_id = {v: i+1 for i, v in enumerate(list_parties)}\n",
    "print(dict_party_rts_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe1a9697",
   "metadata": {},
   "source": [
    "### 1.3. Load dataset\n",
    "This demo uses three fabricated dataset datasets of 5 samples and 2 features each, indexed by unique ID (unsigned int) for each sample. The datasets share common samples (i.e., some IDs appear in more than one dataset), but have different features. This setting corresponds to the Vertical Federated Learning (VFL) model. The combination of the 3 datasets can be viewed as a single virtual dataset of of 10 samples and 6 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86b45cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "num_features = 6\n",
    "samples_uids = random.sample(range(2**32), num_samples)\n",
    "samples_data = np.random.rand(num_samples, num_features)\n",
    "data = pd.DataFrame(samples_data, index=samples_uids, columns=['feature-{}'.format(i+1) for i in range(num_features)])\n",
    "data.index.rename('UIDs', inplace=True)\n",
    "print('samples-{}, features-{}'.format(num_samples, num_features))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6eb031",
   "metadata": {},
   "source": [
    "### 1.4. Split the dataset vertically and assign it to the parties\n",
    "The dataset is partitioned vertically between the parties, while each party gets a different 2 features of the dataset. Also, to make the PSI non-trivial, we give each party a random 5 samples out of the 10 samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0e6a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample_each_party = 5\n",
    "\n",
    "dict_party_data = {}\n",
    "for party_id in range(num_parties):\n",
    "    party_features = num_features//num_parties\n",
    "    if party_id == num_parties-1:\n",
    "        party_features = num_features - (num_features//num_parties) * (num_parties-1)\n",
    "\n",
    "    start_idx = (num_features//num_parties) * party_id\n",
    "    p_data = data.iloc[:, start_idx: start_idx + party_features]\n",
    "\n",
    "    p_data = p_data.sample(n=num_sample_each_party)\n",
    "                \n",
    "    dict_party_data['party-{}'.format(party_id + 1)] = {\n",
    "        'uids': p_data.index.values.tolist(),\n",
    "        'data': p_data\n",
    "    }\n",
    "\n",
    "for p, v in dict_party_data.items():\n",
    "    print('{}-dataset'.format(p))\n",
    "    print(v['data'], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3e32ae",
   "metadata": {},
   "source": [
    "### 1.5. Aggregator setup\n",
    "The aggregator creates Helayers context to be used in the Private Set Intersection protocol and in the Federated Learning algorithm. This context can be of different types, but it can also use the same context for both tasks.\n",
    " \n",
    "Then, it communicates with the parties and sends them the context object (The means by which it does it are out of the scope of this demo). In this demo we will just use the same context object in the parties' code).\n",
    "\n",
    "Finally, the aggregator initialize a AggregatorPsiManager object to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fcb66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "he_context = pyhelayers.HeaanContext()\n",
    "requirements = pyhelayers.HeConfigRequirement(\n",
    "    num_slots = 2 ** 15,\n",
    "    multiplication_depth = 9,\n",
    "    fractional_part_precision = 48,\n",
    "    integer_part_precision = 12,\n",
    "    security_level = 128)\n",
    "requirements.bootstrappable = True\n",
    "\n",
    "he_context.init(requirements)\n",
    "he_context.set_automatic_bootstrapping(True)\n",
    "\n",
    "print(he_context.print_signature())\n",
    "\n",
    "aggregator = pyhelayers.AggregatorPsiManager(he_context, he_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab66cfba",
   "metadata": {},
   "source": [
    "## Step 2 - Run the PSI protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8860ebf",
   "metadata": {},
   "source": [
    "### 2.1. Create encrypted hash tables\n",
    "Each party initializes a RtsPsiManager object with the Helayers context the aggregator sent, with its UIDs and data, and with the shared secret the parties agreed about.\n",
    "\n",
    "Then, they run the first step of the PSI protocol, which is inserting their UIDs into an encrypted hash table. \n",
    "\n",
    "Finally, they send the serialized encrypted hash tables to the other parties, and send a mapping between the original dataset indices to the hash table indices to the aggregator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d789dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_party_psi = {}\n",
    "dict_party_hash = {}\n",
    "dict_party_mapping = {}\n",
    "\n",
    "\n",
    "for party in list_parties:\n",
    "    psi_manager = pyhelayers.RtsPsiManager(\n",
    "        he_context, he_context, \n",
    "        dict_party_rts_id[party], \n",
    "        dict_party_data[party]['uids'], \n",
    "        dict_party_data[party]['data'], \n",
    "        parties_shared_secret)\n",
    "\n",
    "    with utils.elapsed_timer('{} generate its encrypted hash table'.format(party), 1) as timer:\n",
    "        dict_party_hash[party] = psi_manager.insert_to_hash().save_to_buffer()\n",
    "\n",
    "    dict_party_mapping[party] = psi_manager.get_uids_mapping()\n",
    "\n",
    "    dict_party_psi[party] = psi_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7e3fb5",
   "metadata": {},
   "source": [
    "### 2.2. Generate indicator vectors\n",
    "Each party receives the serialized data from the other parties and deserialize it. Then, it proceeds to generate an encrypted indicator vector that indicates which samples are in the intersection, and will be used later in the protocol.\n",
    "\n",
    "Finally, it sends the serialized result back to the party that sent the original hash table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf77838",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_party_indicators = {}\n",
    "\n",
    "# each party generates indicator for the rest parties\n",
    "for party in list_parties:\n",
    "    indicators = {}\n",
    "    psi_manager = dict_party_psi[party]\n",
    "    for other_party in list_parties:\n",
    "        if other_party != party:\n",
    "            rts_id = dict_party_rts_id[other_party]\n",
    "            hash_table = pyhelayers.CTileTensor(he_context)\n",
    "            hash_table.load_from_buffer(dict_party_hash[other_party])\n",
    "\n",
    "            with utils.elapsed_timer('{} generate the indicator vector for {}'.format(party, other_party), 1) as timer:\n",
    "                indicators[other_party] = psi_manager.generate_indicator_vector(rts_id, hash_table).save_to_buffer()\n",
    "            \n",
    "    dict_party_indicators[party] = indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168c309b",
   "metadata": {},
   "source": [
    "### 2.3. The aggregator rearranges the indicator vectors\n",
    "The aggregator uses the mapping sent by Alice to rearrange the indicator vector, so that the order of the indicators will be the same as the original order of the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd485abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_party_rearranged_indicators = {}\n",
    "\n",
    "for party in list_parties:\n",
    "    mapping_party = dict_party_mapping[party]\n",
    "    rearranged_indicators = {}\n",
    "    for other_party in list_parties:\n",
    "        if other_party != party:\n",
    "            indicator = pyhelayers.CTileTensor(he_context)\n",
    "            indicator.load_from_buffer(dict_party_indicators[other_party][party])\n",
    "\n",
    "            with utils.elapsed_timer('aggregator rearrange indicator vector for {}'.format(party), 1) as timer:\n",
    "                rearranged_indicators[other_party] = aggregator.rearrange_indicator_vector(indicator, mapping_party).save_to_buffer()\n",
    "\n",
    "    dict_party_rearranged_indicators[party] = rearranged_indicators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd9667",
   "metadata": {},
   "source": [
    "### 2.3. Parties compact local datasets\n",
    "Finally, each party receives the encrypted rearranged indicators vector from the aggregator. they use it to privately sort their data, such that the first rows are records that are in the intersection, and the rest of the rows are encryptions of 0s (note that the relative order of the samples that are in the intersection is the same as the relative order of them in the DoubleTensor given to the c'tor). The resulted CTileTensor will be then used in the learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f1fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_party_result = {}\n",
    "\n",
    "for party in list_parties:\n",
    "    psi_manager = dict_party_psi[party]\n",
    "    lst_rearranged_indicator = []\n",
    "    lst_rts_id = []\n",
    "    for other_party in list_parties:\n",
    "        if other_party != party:\n",
    "            rearranged_indicator = pyhelayers.CTileTensor(he_context)\n",
    "            rearranged_indicator.load_from_buffer(dict_party_rearranged_indicators[party][other_party])\n",
    "            lst_rearranged_indicator.append(rearranged_indicator)\n",
    "            lst_rts_id.append(dict_party_rts_id[other_party])\n",
    "    indicator_party = pyhelayers.CTileTensorVector(lst_rearranged_indicator)\n",
    "    final_indicator_party = psi_manager.multiply_indicator_vectors(lst_rts_id, indicator_party)\n",
    "\n",
    "    with utils.elapsed_timer('{} local dataset compaction'.format(party), 1) as timer:\n",
    "        dict_party_result[party] = psi_manager.compaction(final_indicator_party)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1397f00",
   "metadata": {},
   "source": [
    "## Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c62f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "expected = reduce(lambda  left,right: pd.merge(left,right,on=['UIDs'],\n",
    "                                            how='inner'), [dict_party_data['party-{}'.format(party_id + 1)]['data'] for party_id in range(num_parties)])\n",
    "\n",
    "tt_encoder = pyhelayers.TTEncoder(he_context)\n",
    "\n",
    "for party in list_parties:\n",
    "    psi_res = tt_encoder.decrypt_decode_double(dict_party_result[party])\n",
    "    expected_res = expected[dict_party_data[party]['data'].columns].reset_index(drop=True).reindex(range(num_sample_each_party)).fillna(0)\n",
    "\n",
    "    print('{}-psi-result'.format(party))\n",
    "    print(pd.DataFrame(psi_res).to_string(header=False, index=False))\n",
    "    print('\\n{}-expected-result'.format(party))\n",
    "    print(expected_res.to_string(header=False, index=False))\n",
    "    print('\\n')\n",
    "\n",
    "    assert np.abs(np.subtract(psi_res, expected_res.to_numpy())).max() < 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848606ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RAM usage:\", utils.get_used_ram(), \"MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

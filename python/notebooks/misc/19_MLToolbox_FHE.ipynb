{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FHE prediction over MLToolbox generated model\n",
    "Expected RAM usage: 40 GB.\n",
    "Expected runtime: 50 minutes.\n",
    "\n",
    "This notebook should be run after running `19_MLToolbox.ipynb` notebook. It loads the model and input samples generated by `19_MLToolbox.ipynb`, and runs prediction on the plain MLToolbox model as well as on an FHE encryption of the MLToolbox model. Finally, the plaintext and FHE prediction outputs are compared to make sure they are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Prediction on plaintext model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Imports and initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pyhelayers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When `debug_mode` flag is on, the FHE prediction is done using a `Mockup` encryption scheme, which holds everything in plaintext but applies the same operations that are applied in case of a real encryption. Setting this flag to `True` will reduce the runtime of the notebook, which is useful for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_mode = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load input sampels\n",
    "We assume that `19_MLToolbox.ipynb` notebook has been run prior to running this block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1\n",
    "plain_samples = torch.load('../outputs/mltoolbox/plain_samples.pt')\n",
    "plain_samples = plain_samples[:num_samples]\n",
    "batch_size = len(plain_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Run prediction on MLToolbox plaintext model\n",
    "We Load the MLToolbox model generated by `19_MLToolbox.ipynb` notebook and run prediction with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(os.path.join('../outputs/mltoolbox/polynomial/resnet18_postproc_checkpoint.pth.tar'), map_location=torch.device('cpu'))\n",
    "model = checkpoint['model']\n",
    "plain_model_predictions = model(plain_samples).detach().numpy()\n",
    "plain_predicted_labels = np.argmax(plain_model_predictions, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. FHE prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load NN architecture and weights using the FHE library\n",
    "\n",
    "We use the `init_from_onnx_file` function of the `NeuralNetPlain` class to load the NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../outputs/mltoolbox/polynomial/resnet18.onnx'\n",
    "nnp = pyhelayers.NeuralNetPlain()\n",
    "hyper_params = pyhelayers.PlainModelHyperParams()\n",
    "nnp.init_from_files(hyper_params,[model_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Compile\n",
    "\n",
    "Using HE can require configuring complex and non-intuitive parameters. Luckily, helayers has an `Optimizer` tool offers an automatic optimization process that analyzes the model, and tunes various HE parameters to work best for the given scenario.\n",
    "\n",
    "The optimizer runs when we run 'compile' on the plain model. It receives also run requirements, some simple and intuitive input from the user (e.g., the desired security level). As output, it produces a `profile` object which contains all the details related to the HE configuration and packing. These details are automatically selected to ensure optimal performance given the user's requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_run_req = pyhelayers.HeRunRequirements()\n",
    "# Use the HEaaN context as the underlying FHE\n",
    "he_run_req.set_he_context_options([pyhelayers.HeaanContext()])\n",
    "# The encryption is at least as strong as 128-bit encryption.\n",
    "he_run_req.set_security_level(128)\n",
    "# Our numbers are theoretically stored with a precision of about 2^-40.\n",
    "he_run_req.set_fractional_part_precision(30)\n",
    "# The model weights are kept in the plain\n",
    "he_run_req.set_model_encrypted(False)\n",
    "# Activate lazy encoding of the weights to save memory\n",
    "he_run_req.set_lazy_encoding(True)\n",
    "\n",
    "if debug_mode:\n",
    "    # In debug mode, we use a fixed tile layout to reduce compilation time\n",
    "    tile_layout = pyhelayers.TTShape([16, 8, 8, 16, 1])\n",
    "    he_run_req.set_fixed_tile_layout(tile_layout)\n",
    "else:\n",
    "    # The batch size for NN.\n",
    "    he_run_req.optimize_for_batch_size(batch_size)\n",
    "\n",
    "# Compile - run the optimizer\n",
    "profile = pyhelayers.HeModel.compile(nnp, he_run_req)\n",
    "\n",
    "profile_as_json = profile.to_string()\n",
    "# Profile supports I/O operations and can be stored on file.\n",
    "print(json.dumps(json.loads(profile_as_json), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Initialize the context, and encrypt the NN\n",
    "Now we initialize the context object and encrypt the neural network using our profile object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug_mode:\n",
    "    profile.set_not_secure_mockup()\n",
    "    \n",
    "context=pyhelayers.HeModel.create_context(profile)\n",
    "nn = pyhelayers.NeuralNet(context)\n",
    "nn.encode(nnp, profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Encrypt the input samples\n",
    "Here, we encrypt the samples we're going to be running an inference on. The data is encrypted by the iop object (input output processor), which contains model meta data only, and can process inputs and outputs of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iop=nn.create_io_processor()\n",
    "encrypted_samples = pyhelayers.EncryptedData(context)\n",
    "iop.encode_encrypt_inputs_for_predict(encrypted_samples, [plain_samples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Run prediction over encrypted data, using the encrypted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypted_predictions = pyhelayers.EncryptedData(context)\n",
    "predict_start = datetime.now()\n",
    "nn.predict(encrypted_predictions, encrypted_samples)\n",
    "predict_end = datetime.now()\n",
    "print('prediction time = %d seconds', predict_end - predict_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Decrypt the prediction result\n",
    "The final labels are computed as the argmax of the predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhe_model_predictions = iop.decrypt_decode_output(encrypted_predictions)\n",
    "fhe_predicted_labels = np.argmax(fhe_model_predictions, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Compare the predictions of the encrypted model with the predictions of the plain model\n",
    "The FHE model's predictions are shown to match those produced by the plain model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('labels predicted by the FHE model: ', fhe_predicted_labels)\n",
    "print('labels predicted by the plain model: ', plain_predicted_labels)\n",
    "np.testing.assert_array_equal(fhe_predicted_labels, plain_predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fhe-py38-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df1dc8a95496e2ea0668c040193b5c5e80cabbb7"
   },
   "source": [
    "# Neural Network Inference for Fraud Detection Using FHE (simple)\n",
    "\n",
    "**NOTE:**\n",
    "\n",
    "This notebook is very similar to the previous Neural Network Fraud Detection example in Notebook 02, but uses a single line of FHE code and is much more simple. Unlike Notebook 02, you do not have to call the Optimizer, specify different parameters or encode and encrypt; you only call a single FHE command using the pyhelayers extension API. \n",
    "\n",
    "This example demonstrates the *pyhelayersext API*, which offers an easy integration with the keras library and replaces the keras predictions with the FHE implementation. The FHE configuration details are taken from fhe.json configuration file. This config file contains FHE parameters that the user can tune (e.g. batch size, security level, etc.).\n",
    "\n",
    "#### This demo uses the Credit Card Fraud Detection dataset, originally taken from: https://www.kaggle.com/mlg-ulb/creditcardfraud \n",
    "This dataset contains actual anonymized transactions made by credit card holders from September 2013 and is labeled for transactions being fraudulent or genuine. See references at the bottom of the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Step 1. Import different machine learning libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a391e57ea916bce5bb32ccb58455f379a925a170"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import utils \n",
    "\n",
    "utils.verify_memory()\n",
    "\n",
    "##### For reproducibility\n",
    "seed_value= 1\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "#####\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "import h5py\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "#####\n",
    "# import utils\n",
    "import sys\n",
    "path_to_utils='.'\n",
    "sys.path.append(path_to_utils)\n",
    "import utils\n",
    "# import activations\n",
    "sys.path.append(os.path.join(path_to_utils, 'data_gen'))\n",
    "from activations import SquareActivation\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 32 \n",
    "optimizer = Adam\n",
    "lr = 0.01\n",
    "print(\"misc. init complete\")\n",
    "\n",
    "PATH = os.path.join(utils.get_data_sets_dir(), 'net_fraud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Step 2. Read the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(utils.get_data_sets_dir(path_to_utils), 'net_fraud', 'creditcard.csv'))\n",
    "\n",
    "print(f'Reading {df.shape[0]} samples')\n",
    "\n",
    "X = df.loc[:, df.columns.tolist()[1:30]].values\n",
    "Y = df.loc[:, 'Class'].values\n",
    "print(f'number of features: {X.shape[1]}')\n",
    "\n",
    "X = preprocessing.normalize(X)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, stratify=Y, random_state=0)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.33, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Step 3. Replicate the smallest class for balancing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replicate_smallest_class(x, y, class_id):\n",
    "        y_fraud_list = y[y == class_id]\n",
    "        x_fraud_list = x[y == class_id]\n",
    "\n",
    "        for _ in range(5):\n",
    "            copy_fraudlist = np.copy(x_fraud_list)\n",
    "            y_fraud_copy = np.copy(y_fraud_list)\n",
    "            x = np.concatenate((x, copy_fraudlist))\n",
    "            y = np.concatenate((y, y_fraud_copy))\n",
    "\n",
    "        permut = np.random.permutation(x.shape[0])\n",
    "        x = x[permut]\n",
    "        y = y[permut]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "x_train, y_train = replicate_smallest_class(x_train, y_train, class_id=1)\n",
    "\n",
    "nb_train_samples = (x_train.shape[0] // batch_size) * batch_size\n",
    "x_train = x_train[:nb_train_samples]\n",
    "y_train = y_train[:nb_train_samples]\n",
    "\n",
    "\n",
    "print(\"After replicating items from the smaller class:\")\n",
    "print(f'x_train: {x_train.shape}')\n",
    "print(f'x_val: {x_val.shape}')\n",
    "print(f'x_test: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Step 4. Reshape labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(y_train.shape[0], -1)\n",
    "y_val = y_val.reshape(y_val.shape[0], -1)\n",
    "y_test = y_test.reshape(y_test.shape[0], -1)\n",
    "\n",
    "print(\"Training data ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Step 5. Save the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_set(x, y, data_type, s=''):\n",
    "    print(\"Saving x_{} of shape {}\".format(data_type, x.shape))\n",
    "    xf = h5py.File(os.path.join(PATH, f'x_{data_type}{s}.h5'), 'w')\n",
    "    xf.create_dataset('x_{}'.format(data_type), data=x)\n",
    "    xf.close()\n",
    "\n",
    "    yf = h5py.File(os.path.join(PATH, f'y_{data_type}{s}.h5'), 'w')\n",
    "    yf.create_dataset(f'y_{data_type}', data=y)\n",
    "    yf.close()\n",
    "    \n",
    "save_data_set(x_test, y_test, data_type='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Step 6. Fraud detection network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(20, input_shape=(x_train.shape[1],)))\n",
    "model.add(SquareActivation())\n",
    "model.add(Dense(5))\n",
    "model.add(SquareActivation())\n",
    "model.add(Dense(1))\n",
    "model.add(SquareActivation())\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer(learning_rate=lr),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Step 7. Train the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=2,\n",
    "              validation_data=(x_val, y_val),\n",
    "              shuffle=True,\n",
    "              )\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Test loss: {score[0]:.3f}')\n",
    "print(f'Test accuracy: {score[1] * 100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Step 8. Define the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "x_test = x_test[0:batch_size,:]\n",
    "y_test = y_test[0:batch_size,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix - TEST\n",
    "After the replacement below instead of regular keras code runs code predicting on encrypted data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Step 9. Perform FHE prediction\n",
    "This is the only line of code that deals with FHE! The only thing you need to add is the pyhelayers extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict = __import__('pyhelayers.ext').ext.replace(model.predict, config_file='./fhe.json')\n",
    "utils.start_timer()\n",
    "y_pred_vals = model.predict(x_test)\n",
    "y_pred = (y_pred_vals > 0.5).astype(np.int32)\n",
    "utils.end_timer(\"NN prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Step 10. Test the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,t,thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(f\"AUC Score: {metrics.auc(f,t):.3f}\")\n",
    "print(\"Classification report:\")\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "References:\n",
    "\n",
    "<sub><sup> 1.\tAndrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015 </sup></sub>\n",
    "\n",
    "<sub><sup> 2.\tDal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon </sup></sub>\n",
    "\n",
    "<sub><sup> 3.\tDal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE </sup></sub>\n",
    "\n",
    "<sub><sup> 4.\tDal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi) </sup></sub>\n",
    "\n",
    "<sub><sup> 5.\tCarcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-Aël; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier </sup></sub>\n",
    "\n",
    "<sub><sup> 6.\tCarcillo, Fabrizio; Le Borgne, Yann-Aël; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing </sup></sub>\n",
    "\n",
    "<sub><sup> 7.\tBertrand Lebichot, Yann-Aël Le Borgne, Liyun He, Frederic Oblé, Gianluca Bontempi Deep-Learning Domain Adaptation Techniques for Credit Cards Fraud Detection, INNSBDDL 2019: Recent Advances in Big Data and Deep Learning, pp 78-88, 2019 </sup></sub>\n",
    "\n",
    "<sub><sup> 8.\tFabrizio Carcillo, Yann-Aël Le Borgne, Olivier Caelen, Frederic Oblé, Gianluca Bontempi Combining Unsupervised and Supervised Learning in Credit Card Fraud Detection Information Sciences, 2019 </sup></sub>\n",
    "\n",
    "<sub><sup> 9.\tYann-Aël Le Borgne, Gianluca Bontempi Machine Learning for Credit Card Fraud Detection - Practical Handbook </sup></sub>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cefe17",
   "metadata": {},
   "source": [
    "## Inference under FHE for the MNIST Dataset using helayers\n",
    "\n",
    "In this demo, we'll deal with a classification problem for the MNIST dataset [1], trying to correctly classify a batch of samples using a neural network model that will be created and trained using the Keras library (with architecture similar to reference [2]).\n",
    "First, we'll build a plain neural network for the MNIST model. Then, we'll encrypt the trained network and run inference over it using FHE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf88e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "##### For reproducibility\n",
    "seed_value= 1\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras import utils, losses\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import h5py\n",
    "\n",
    "# import activations\n",
    "import sys\n",
    "sys.path.append(os.path.join('.', 'data_gen'))\n",
    "from activations import SquareActivation\n",
    "\n",
    "PATH = os.path.join('data', 'net_mnist')\n",
    "if not os.path.exists(PATH):\n",
    "    os.makedirs(PATH)\n",
    "\n",
    "batch_size = 500\n",
    "epochs = 10\n",
    "print(\"Misc. initializations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5bb3b0",
   "metadata": {},
   "source": [
    "### Load and Preprocess the MNIST Dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c243f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('data ready')\n",
    "\n",
    "# Image padding\n",
    "x_train = np.pad(x_train, ((0, 0), (0, 1), (0, 1), (0, 0)))\n",
    "x_test = np.pad(x_test, ((0, 0), (0, 1), (0, 1), (0, 0)))\n",
    "print('Added padding. New shape: ',x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c161e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation data\n",
    "testSize=16\n",
    "x_val = x_test[:-testSize]\n",
    "x_test = x_test[-testSize:]\n",
    "y_val = y_test[:-testSize]\n",
    "y_test = y_test[-testSize:]\n",
    "print('Validation and test data ready')\n",
    "\n",
    "# Convert class vector to binary class matrices\n",
    "num_classes = 10\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "y_val = utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "input_shape = x_train[0].shape\n",
    "print(f'input shape: {input_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5284f27b",
   "metadata": {},
   "source": [
    "### Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969954aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_set(x, y, data_type, s=''):\n",
    "    fname=os.path.join(PATH, f'x_{data_type}{s}.h5')\n",
    "    print(\"Saving x_{} of shape {} in {}\".format(data_type, x.shape,fname))\n",
    "    xf = h5py.File(fname, 'w')\n",
    "    xf.create_dataset('x_{}'.format(data_type), data=x)\n",
    "    xf.close()\n",
    "\n",
    "    yf = h5py.File(os.path.join(PATH, f'y_{data_type}{s}.h5'), 'w')\n",
    "    yf.create_dataset(f'y_{data_type}', data=y)\n",
    "    yf.close()\n",
    "\n",
    "save_data_set(x_test, y_test, data_type='test')\n",
    "save_data_set(x_train, y_train, data_type='train')\n",
    "save_data_set(x_val, y_val, data_type='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8a6ed1",
   "metadata": {},
   "source": [
    "### Build a Plain Neural Network for the MNIST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27228c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=5, kernel_size=5, strides=2, padding='valid',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Flatten())\n",
    "model.add(SquareActivation())\n",
    "model.add(Dense(100))\n",
    "model.add(SquareActivation())\n",
    "model.add(Dense(num_classes))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3829f0",
   "metadata": {},
   "source": [
    "### Train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609e5220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squared_error(y_true, y_pred):\n",
    "    return K.sum(K.square(y_pred - y_true), axis=-1)\n",
    "\n",
    "model.compile(loss=sum_squared_error,\n",
    "                  optimizer='Adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=2,\n",
    "              validation_data=(x_val, y_val),\n",
    "              shuffle=True,\n",
    "              )\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Test loss: { score[0]:.3f}')\n",
    "print(f'Test accuracy: {score[1] * 100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2477a1b2",
   "metadata": {},
   "source": [
    "### Report the Confusion Matrix of the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf299e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_pred_vals = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred_vals, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e4b519",
   "metadata": {},
   "source": [
    "### Serialize Model and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82393d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(os.path.join(PATH, 'model.json'), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(os.path.join(PATH, 'model.h5'))\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9211fd0b",
   "metadata": {},
   "source": [
    "We are all done training the plain network. Next we will encrypt the network and run inference over it using FHE. Let's start with some initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee4130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhelayers\n",
    "import utils\n",
    "\n",
    "utils.verify_memory()\n",
    "\n",
    "print('Misc. initalizations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd942b0e",
   "metadata": {},
   "source": [
    "Here's a general outline of what we're about to do:\n",
    "\n",
    "First, we'll build a plain neural network model, loading it from files that were created and saved above.\n",
    "Then, an automated optimizer will examine our network and will provide us with an HE profile: various configuration details that will allow running inference under encryption efficiently.\n",
    "Finally, we'll construct a neural net object: an encrypted version of our network. We'll demonstrate how we can encrypt data, run inference on our encrypted network, and compare the results against the expected labels.\n",
    "Now let's dive in . . .\n",
    "\n",
    "First, we define a variable \"context\" to represent the library. Note that it is still empty and will only be initialized later, once we know what configuration should we use depending on the NN model we'll build and on our requirements of the inference process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e11341",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = pyhelayers.DefaultContext()\n",
    "print('HE context ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56683410",
   "metadata": {},
   "source": [
    "We would like to create our NN model encrypted under HE. We'll first build a plain NN model and then encrypt it to receive an encrypted version of the model. We'll declare a variable \"nnp\" to represent the plain NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13139d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnp = pyhelayers.NeuralNetPlain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c249f1d7",
   "metadata": {},
   "source": [
    "Next, we initialize the NN architecture and weights. The architecture describes the different layers that form the NN and information on each. We'll load it from the JSON file that was created while building and training the model. We demonstrate how to load the weights from a pre-defined HDF5 file created in Python, containing the weights in the plain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eebd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = pyhelayers.PlainModelHyperParams()\n",
    "nnp.init_from_files(hyper_params, [os.path.join(PATH, \"model.json\"), os.path.join(PATH, \"model.h5\")])\n",
    "print(\"loaded plain model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1528e0",
   "metadata": {},
   "source": [
    "And now for the interesting part. We'll build an encrypted version of our NN model. But in order to do that, we first need to define a configuration of the library and of the layout of each ciphertext, which we call a \"tile\".  \n",
    "Defining all the right parameters for the library and for the layout is some hard work. It is complex and subtle, since the configuration includes multiple HE-related parameters, many of them depend on one another and on the specific NN model we defined, and the values we choose will have great influence on the security, accuracy and performance of the inference process.  \n",
    "In order to avoid this complexity, we'll use the HE profile optimizer. This optimizer helps us find a configuration that's guaranteed to be secure and feasible. Furthermore, the optimizer receives the plain NN we've built and considers what's optimal for this very model in terms of performance.  \n",
    "We can notify the optimizer of various requirements we have for running the model, with respect to the library and packaging considerations. For example here we ask to optimize for a batch size of 16 samples.\n",
    "The optimizer is called when compiling the model, given HE run requirements and the plain model.\n",
    "The result returned is a model \"profile\", describing how the library and the encrypted NN should be initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baaa636",
   "metadata": {},
   "outputs": [],
   "source": [
    "he_run_req = pyhelayers.HeRunRequirements()\n",
    "he_run_req.set_he_context_options([pyhelayers.DefaultContext()])\n",
    "he_run_req.optimize_for_batch_size(16)\n",
    "\n",
    "profile = pyhelayers.HeModel.compile(nnp, he_run_req)\n",
    "batch_size = profile.get_optimal_batch_size()\n",
    "print('Profile ready. Batch size=',batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac516603",
   "metadata": {},
   "source": [
    "The HE profile includes a set of requirements from the library, such as the security level, precision, size of ciphertext, multiplication depth etc. Some of these can be set by the user and some others were found by the optimizer based on the NN architecture we defined. We'll now take this requirement object and use it to initialize the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1730768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = pyhelayers.HeModel.create_context(profile)\n",
    "print('HE context initalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35315c98",
   "metadata": {},
   "source": [
    "We will now build our encrypted NN. We start by defining an empty HE NN, providing it with the library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bea68e1",
   "metadata": {},
   "source": [
    "Now we initialize the NN and populate its weights. Since we have already built a plain NN describing the model architecture and containing the weights, we'll simply encrypt into an encrypted version of the NN. Notice we provide the profile that was recommended by the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b268623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = pyhelayers.NeuralNet(context)\n",
    "nn.encode_encrypt(nnp, profile)\n",
    "print('Encrypted network ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91afd062",
   "metadata": {},
   "source": [
    "We will now load real samples of the MNIST dataset to classify. We will load the samples and the corresponding true labels from HDF5 files. We will also extract the first batch of samples and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb16a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(os.path.join(PATH, \"x_test.h5\")) as f:\n",
    "    x_test = np.array(f[\"x_test\"])\n",
    "with h5py.File(os.path.join(PATH, \"y_test.h5\")) as f:\n",
    "    y_test = np.array(f[\"y_test\"])\n",
    "    \n",
    "plain_samples, labels = utils.extract_batch(x_test, y_test, batch_size, 0)\n",
    "\n",
    "print('Batch of size',batch_size,'loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a4eee5",
   "metadata": {},
   "source": [
    "To populate the input, we need to encode and then encrypt the values of the plain input under HE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8a882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iop = nn.create_io_processor()\n",
    "samples = pyhelayers.EncryptedData(context)\n",
    "iop.encode_encrypt_inputs_for_predict(samples, [plain_samples])\n",
    "print('Test data encrypted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59df1642",
   "metadata": {},
   "source": [
    "We now go ahead with the inference itself. We run the encrypted input through the encrypted NN to obtain encrypted results. This computation does not use the secret key and acts on completely encrypted values. Running the inference is done using the \"predict\" method of the NN, that receives the destination 3D structure to put the result of the computation in, and the input for the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f4d6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.start_timer()\n",
    "\n",
    "predictions = pyhelayers.EncryptedData(context)\n",
    "nn.predict(predictions, samples)\n",
    "\n",
    "duration=utils.end_timer('predict')\n",
    "utils.report_duration('predict per sample',duration/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ecf2a9",
   "metadata": {},
   "source": [
    "In order to assess the results of the computation, we first need to decrypt them. This is done by an IO processor that has the secret key and is capable of decrypting the ciphertext and decoding it into plaintext version of the HE computation result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17629ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_predictions = iop.decrypt_decode_output(predictions)\n",
    "print('predictions',plain_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d884b6a5",
   "metadata": {},
   "source": [
    "Now we compare the results against the expected labels and compute the confusion matrix and the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.assess_results(labels, plain_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae69f9e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "References:\n",
    "\n",
    "<sub><sup> 1.\tLeCun, Yann and Cortes, Corinna. \"MNIST handwritten digit database.\" (2010): </sup></sub>\n",
    "\n",
    "<sub><sup> 2.\tGilad-Bachrach, R., Dowlin, N., Laine, K., Lauter, K., Naehrig, M. &amp; Wernsing, J.. (2016). CryptoNets: Applying Neural Networks to Encrypted Data with High Throughput and Accuracy. Proceedings of The 33rd International Conference on Machine Learning, in Proceedings of Machine Learning Research 48:201-210 Available from https://proceedings.mlr.press/v48/gilad-bachrach16.html.\n",
    "</sup></sub>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

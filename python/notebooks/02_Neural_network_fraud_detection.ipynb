{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc19ac6f",
   "metadata": {},
   "source": [
    "# Neural Network Inference for Fraud Detection Using FHE\n",
    "expected RAM usage: 6.2GB  \n",
    "expected runtime: 14 seconds.\n",
    "\n",
    "## Introduction\n",
    " \n",
    "This example demonstrates a use case in the finance domain as well as demonstrating encrypted machine learning. We will demonstrate how we can use FHE along with neural networks (NN) to carry out predictions for fraud detection while keeping the data, the NN model and the prediction results encrypted at all times. The neural network and dataset determine fraudulent activities based on anonymized transactions. \n",
    "\n",
    "This example showcases how you are able to utilize the processing power of an untrusted environment while preserving the privacy of your sensitive data. The demonstration is split into a privileged client that has access to unencrypted data and models, and an unprivileged server that only performs homomorphic computation in a completely encrypted fashion. The data and the NN model are encrypted in a trusted client environment and then are used to carry out predictions in an untrusted or public environment. Finally, the prediction results return encrypted and can only be decrypted by the data owner in the trusted environment. The concept of providing fully outsourced, but fully encrypted computation to a cloud provider is a major motivating factor in the field of FHE. This use case example shows the capability of the SDK to build such applications.\n",
    "\n",
    "**NOTE: while the client and server are not literally separated (nor demonstrating true remote cloud computation), the concepts generalize. One can imagine running the trusted code on local environment and the prediction code on a less trusted environment like the cloud. Additionally, we are working on FHE cloud that simplifies a lot of this.**\n",
    "\n",
    "#### This demo uses the Credit Card Fraud Detection dataset, originally taken from: https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "This dataset contains actual anonymized transactions made by credit card holders from September 2013 and is labeled for transactions being fraudulent or genuine. See references at the bottom of the page.\n",
    "\n",
    "## Use case\n",
    "\n",
    "Global credit card fraud is expected to reach $35B by 2025 (Nilson Report, 2020) and since the beginning of the COVID-19 pandemic, 40% of financial services firms saw an increase in fraudulent activity (LIMRA, 2020). As well as volume effects, COVID-19 has worsened the false positive issue for over two-thirds of institutions (69%). A key challenge for many institutions is that significant changes in consumer behavior have often resulted in existing fraud detection systems wrongly identifying legitimate behavior as suspected fraud (Omdia, 2021).\n",
    "\n",
    "With FHE, you are now able to unlock the value of regulated and sensitive PII data in the context of a less trusted cloud environment by performing AI, machine learning, and data analytic computations without ever having to decrypt. By training your AI models with additional sensitive data, you are able to achieve higher accuracy in fraud detection and reduce the false positive rate while also utilizing the many benefits of cloud computing.\n",
    "\n",
    "FHE can also help to support a zero trust strategy and can implement strong access control measures by keeping the data, the models that process the data and the results generated encrypted and confidential; only the data owner has access to the private key and has the privilege to decrypt the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12be9484",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Step 1. Load the existing model and dataset into the trusted environment\n",
    "\n",
    "In this step we are loading a pre-trained model and a dataset while operating on a trusted client environment. The model and data used in this notebook correspond to a credit card fraud dataset. \n",
    "\n",
    "For convenience, the model has been pre-trained and is available in the `data_gen` folder, but you can also experiment with the model you generate yourself. To do that, first run the notebook at `data_gen/fraud_detection_demo.ipynb`, then turn off the boolean flag below, and continue with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ac59ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils \n",
    "\n",
    "utils.verify_memory()\n",
    "\n",
    "load_from_pre_prepared = True\n",
    "\n",
    "from pathlib import Path\n",
    "if load_from_pre_prepared:\n",
    "    INPUT_DIR = Path(utils.get_data_sets_dir()) / 'net_fraud'\n",
    "else:\n",
    "    INPUT_DIR = Path('data/net_fraud/')\n",
    "\n",
    "import json\n",
    "import utils\n",
    "\n",
    "X_H5 = INPUT_DIR / 'x_test.h5'\n",
    "Y_H5 = INPUT_DIR / 'y_test.h5'\n",
    "MODEL_JSON = str(INPUT_DIR / 'model.json')\n",
    "MODEL_H5 = str(INPUT_DIR / 'model.h5')\n",
    "\n",
    "batch_size = 4096\n",
    "plain_samples, labels = utils.extract_batch_from_files(X_H5, Y_H5, batch_size, 0)\n",
    "\n",
    "print('Loaded samples of shape',plain_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1587ed8d",
   "metadata": {},
   "source": [
    "The following figure illustrates the model being used:\n",
    "\n",
    "![Model](img/model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebfdca2",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Step 2. Encrypt the neural network in the trusted environment\n",
    "\n",
    "The next set of steps include the following:\n",
    "\n",
    "#### 2.1. Load NN architecture and weights using the FHE library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df72caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhelayers\n",
    "\n",
    "hyper_params = pyhelayers.PlainModelHyperParams()\n",
    "neural_net_plain = pyhelayers.NeuralNetPlain()\n",
    "neural_net_plain.init_from_files(hyper_params, [MODEL_JSON, MODEL_H5])\n",
    "\n",
    "print('neural_net_plain created and initialized')\n",
    "\n",
    "# NeuralNetPlain hold a neural network that is not encrypted.\n",
    "# It can handle fully-connected, convolutional, and mean-pool layers.\n",
    "# Supported activations: square, and higher-degree polynomials.\n",
    "# It can also be used to run predictions, and compare it with the encrypted model (e.g. for debugging and testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a097ccf-5796-4ac0-be56-656710b7771c",
   "metadata": {},
   "source": [
    "#### 2.2. Compile the plain model\n",
    "\n",
    "Now we take the plain model and run a process called compilation. This runs internally an Optimizer that finds the best parameters for this model. Not only does the Optimizer find the best parameters for you, but it also gives you estimations on the time it would take to predict using a single core, the precision, the memory, the time it would take to encrypt/decrypt, etc. \n",
    "\n",
    "The input to the compilation process are some preferences that we have. In this demo:\n",
    "* We use HEaaN as the underlying backend. It is somewhat faster and takes less memory in this use case.\n",
    "* We choose the batch size, how many samples would you provide each time for the inference model to do the classification\n",
    "\n",
    "This step doesn't yet encrypt the model, but prepares a 'profile' object that we can later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bead528",
   "metadata": {},
   "outputs": [],
   "source": [
    "he_run_req = pyhelayers.HeRunRequirements()\n",
    "# Request a HEaaN context if available, or a SEAL context otherwise\n",
    "he_run_req.set_he_context_options([pyhelayers.HeContext.create([\"HEaaN_CKKS\", \"SEAL_CKKS\"])])\n",
    "\n",
    "# Largest number we'll be able to safely process under encryption is `2^7=128`.\n",
    "he_run_req.set_integer_part_precision(7)\n",
    "# Our numbers are theoretically stored with precision of about 2^-30.\n",
    "he_run_req.set_fractional_part_precision(30)\n",
    "# Batch size for NN.\n",
    "he_run_req.optimize_for_batch_size(batch_size)\n",
    "\n",
    "profile = pyhelayers.HeModel.compile(neural_net_plain, he_run_req)\n",
    "\n",
    "batch_size = profile.get_optimal_batch_size()\n",
    "print(profile.to_string())\n",
    "\n",
    "print(\"He profile ready\")\n",
    "print(\"Batch size: \",batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9fb102",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "\n",
    "* We specify integer precision of `7` bits. If we exceed this number, the results may be erroneous. But remember, no checks can be made when performing computations under encryption as no information on the data can be exposed.\n",
    "\n",
    "* We specify a fractional part of `30` bits. This means our numbers are theoretically stored with precision of about `2^-30`. This is misleading however: the ciphertext will inherently add some noise, and it will gradually corrupt more and more bits. The larger number we set here, the more precise our results will be, but at a cost of more expensive computation.\n",
    "\n",
    "We can specify whatever values we'd like. The optimizer will make sure to notify us if there's no feasible configuration given our requirements and the NN architecture we provide.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199e80ba-ad94-45ef-98ff-5e7b0ed0b1ed",
   "metadata": {},
   "source": [
    "#### 2.3. Initialize the context\n",
    "\n",
    "Here we initialize the FHE library based on the paramaters chosen for us in the profile object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1480edb7-54b4-4e10-8b66-b7a3ee2310ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this to run over a mockup context\n",
    "# profile.set_not_secure_mockup()\n",
    "\n",
    "client_context = pyhelayers.HeModel.create_context(profile)\n",
    "print('Crypto-library ready',client_context.print_signature())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340214fe",
   "metadata": {},
   "source": [
    "#### 2.4. Encrypt the NN\n",
    "\n",
    "The HE profile includes a set of requirements from the library, such as the security level, precision, size of ciphertext, multiplication depth, and some more complicated parameters. They were chosen by the optimizer to provide the best performance for the given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b30fe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_nn = pyhelayers.NeuralNet(client_context)\n",
    "client_nn.encode_encrypt(neural_net_plain, profile)\n",
    "\n",
    "print('client_nn initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5593a77-cac2-4510-aa1e-8e67b29ff12d",
   "metadata": {},
   "source": [
    "#### 2.5. Encrypt the data samples\n",
    "\n",
    "To encrypt the data we first create an io processor (iop for short).\n",
    "The iop object is a lightweight object that knows the model's metadata and can be used to encrypt data for it, and later decrypt the output it sends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d8c51c-044b-45a5-899a-b0dac64994c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iop = client_nn.create_io_processor()\n",
    "client_samples = pyhelayers.EncryptedData(client_context)\n",
    "iop.encode_encrypt_inputs_for_predict(client_samples, [plain_samples])\n",
    "print('Batch encrypted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfc1d36-b248-4c74-b91b-6a4d82babf93",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.6. Save and send\n",
    "We save the encrypted model, the context, and the samples in preparation for sending them to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1873c34c-d453-4cf4-9f76-034ebb4386a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_buffer = client_nn.save_to_buffer()\n",
    "samples_buffer = client_samples.save_to_buffer()\n",
    "\n",
    "# Save the context. Note that this saves all the HE library information, including the \n",
    "# public key, allowing the server to perform HE computations.\n",
    "# The secret key is not saved here, so the server won't be able to decrypt.\n",
    "# The secret key is never stored unless explicitly requested by the user using the designated \n",
    "# method.\n",
    "context_buffer = client_context.save_to_buffer()\n",
    "\n",
    "print('Context, model, and samples saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbbf7d0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Step 3. Perform predictions in the untrusted server using encrypted data and neural network\n",
    "\n",
    "#### 3.1. Load the neural network, samples and context in the server\n",
    "We first load all the data sent from the client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc04f598",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_context = pyhelayers.load_he_context(context_buffer)\n",
    "server_nn = pyhelayers.load_he_model(server_context, nn_buffer)\n",
    "server_samples = pyhelayers.load_encrypted_data(server_context, samples_buffer)\n",
    "print('server ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f08b71",
   "metadata": {},
   "source": [
    "#### 3.2. Perform inference in cloud/server using encrypted data and encrypted NN\n",
    "\n",
    "We can now run the inference of the encrypted data and encrypted NN to obtain encrypted results. This computation does not use the secret key and acts on completely encrypted values.\n",
    "\n",
    "**NOTE: the data, the NN and the results always remain in encyrpted state, even during computation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898d5bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_predictions = pyhelayers.EncryptedData(server_context)\n",
    "with utils.elapsed_timer('predict', batch_size) as timer:\n",
    "    server_nn.predict(server_predictions, server_samples)\n",
    "predictions_buffer = server_predictions.save_to_buffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b19e23a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Step 4. Decrypt the prediction results in the trusted environment\n",
    "\n",
    "Now we're back on the trusted side. We can load the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f93b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_predictions = pyhelayers.load_encrypted_data(client_context,predictions_buffer)\n",
    "print('predictions loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce5c404",
   "metadata": {},
   "source": [
    "Now we can decrypt and decode them. The client's side context also has the secret key, so all objects on the client side can perform decryption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3168ca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_predictions = iop.decrypt_decode_output(client_predictions)\n",
    "print('predictions',plain_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3763e886",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Step 5. Assess the results - precision, recall, F1 score\n",
    "\n",
    "As this classification problem is a binary one, we will assess the results by comparing the positive and negative classifications with the true labels, also calculating the precision, recall and F1 score.\n",
    "\n",
    "When running the model in the plain (see `data_gen/fraud_detection_demo.ipynb`), we get the following confusion matrix:  \n",
    "[[4087 1]  \n",
    " [1 &emsp; 7]].  \n",
    "Comparing the plain results with the confusion matrix reported below shows that the FHE model produces the same results as the plain one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.assess_results(labels, plain_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b080cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RAM usage:\", utils.get_used_ram(), \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4872bf8d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "References:\n",
    "\n",
    "<sub><sup> 1.\tAndrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015 </sup></sub>\n",
    "\n",
    "<sub><sup> 2.\tDal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon </sup></sub>\n",
    "\n",
    "<sub><sup> 3.\tDal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE </sup></sub>\n",
    "\n",
    "<sub><sup> 4.\tDal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi) </sup></sub>\n",
    "\n",
    "<sub><sup> 5.\tCarcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-Aël; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier </sup></sub>\n",
    "\n",
    "<sub><sup> 6.\tCarcillo, Fabrizio; Le Borgne, Yann-Aël; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing </sup></sub>\n",
    "\n",
    "<sub><sup> 7.\tBertrand Lebichot, Yann-Aël Le Borgne, Liyun He, Frederic Oblé, Gianluca Bontempi Deep-Learning Domain Adaptation Techniques for Credit Cards Fraud Detection, INNSBDDL 2019: Recent Advances in Big Data and Deep Learning, pp 78-88, 2019 </sup></sub>\n",
    "\n",
    "<sub><sup> 8.\tFabrizio Carcillo, Yann-Aël Le Borgne, Olivier Caelen, Frederic Oblé, Gianluca Bontempi Combining Unsupervised and Supervised Learning in Credit Card Fraud Detection Information Sciences, 2019 </sup></sub>\n",
    "\n",
    "<sub><sup> 9.\tYann-Aël Le Borgne, Gianluca Bontempi Machine Learning for Credit Card Fraud Detection - Practical Handbook </sup></sub>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
